{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ddb (for Docker Devbox)","text":"<p>Erase environment differences, make developers happy !</p> <p>ddb automates application configuration so differences between development, staging and production environment can  be erased. It provides features to generate, activate and adjust configuration files based on a single overridable  and extendable configuration, while enhancing the developer experience and reducing manual operations.</p> <p>Primarly designed for docker-compose and docker-devbox,  this tool makes the developer forget about the docker hard stuff by providing docker binaries right into it's  computer PATH, so it's experience looks like everything is native and locally installed.</p> <p>Thanks to a pluggable, event based and easy to extend architecture, it can bring powerful configuration automation to  any technical context.</p>"},{"location":"#install","title":"Install","text":"<p>ddb should most often be installed by docker-devbox</p> <p>You should better install the whole docker-devbox toolkit  to enjoy the experience.</p> <p>docker-devbox automatically installs ddb as a  dependency, along some helper docker containers.</p> <p>Only advanced users should install ddb on their own. If you are not sure what to do, do not install ddb  on your own, but follow docker-devbox installation docs.</p> <p>ddb is supported on Linux, Windows and MacOS. </p> <p>You can download binary releases on github, or  install on Python 3.5+ with pip.</p> <pre><code>pip install docker-devbox-ddb\n</code></pre>"},{"location":"binaries/","title":"Binaries","text":"<p>a typical ddb project is configured to register binaries inside the environment.</p> <p>Most often, binaries are registered inside <code>docker-compose.yml.jsonnet</code> using ddb.Binary().</p> <p>Docker binary shims are available in the shell <code>PATH</code> and acts as aliases to binaries living in images or containers of docker-compose services. They delegate to ddb run command to generate a complex <code>docker-compose run</code> or <code>docker-compose exec</code> command to invoke. </p> <p>Those binaries let you think they are locally installed on your computer, but everything still runs inside docker. This is because ddb take cares of common docker pitfalls : project and current working directory are mapped properly,  host shell is integrated and permission issues are solved thanks to fixuid feature.</p> <p>Binary shims are created inside <code>.bin</code> directory of the project on ddb configure command to be registered in the shell <code>PATH</code> during ddb activate command.</p> <p>Binaries can also be registered globally, using ddb.Binary(..., global=true).  Those binaries are then available from any directory. It can be handy to register some global tools using a ddb  project as a way to package it.</p> <p>Binaries can also be registered through shell feature Aliases Management.</p> <p>Customize docker-compose run options for docker binaries</p> <p>You can customize docker-compose run options by using <code>DDB_RUN_OPTS</code> environment variable. It can be used define  environment variables to the container running the binary using docker-compose <code>-e</code> flag.</p> <pre><code># To run phpunit with code coverage, XDEBUG_MODE environment variable has to be set to \"coverage\"\nDDB_RUN_OPTS=\"-e XDEBUG_MODE=coverage\" bin/phpunit\n</code></pre>"},{"location":"commands/","title":"Commands","text":"<p>If you run ddb with no argument, the usage is displayed and showcase available commands.</p> <p>ddb usage</p> <pre><code>usage: ddb [-h] [-v] [-vv] [-s] [-x] [-c] [-w] [-ff] [--version]\n           {init,configure,download,features,config,info,self-update,run,activate,deactivate,check-activated}\n           ...\n\npositional arguments:\n  {init,configure,download,features,config,info,self-update,run,activate,deactivate,check-activated}\n                        Available commands\n    init                Initialize the environment\n    configure           Configure the environment\n    download            Download files from remote sources\n    features            List enabled features\n    config              Display effective configuration\n    info                Display useful information\n    self-update         Update ddb to latest version\n    run                 Display command to run project binary\n    activate            Write a shell script to be executed to activate\n                        environment\n    deactivate          Write a shell script to be executed to deactivate\n                        environment\n    check-activated     Check if project is activated in current shell\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --verbose         Enable more logs\n  -vv, --very-verbose   Enable even more logs\n  -s, --silent          Disable all logs\n  -x, --exceptions      Display exceptions on errors\n  -c, --clear-cache     Clear all used caches\n  -w, --watch           Enable watch mode (hot reload of generated files)\n  -ff, --fail-fast      Stop on first error\n  --version             Display the ddb version and check for new ones.\n</code></pre> <p>Positional argument match command names.</p> <p>Some optional arguments are available globally, regardless the command. These are placed before the command name.</p> <p>Some commands support additional arguments that can be listed with <code>--help</code> flag after the command name.  Those are placed after the command name.</p>"},{"location":"commands/#ddb-configure","title":"ddb configure","text":"<p>Configure the project by scanning project files and performing actions supported by all features.</p> <pre><code>optional arguments:\n  -h, --help  show this help message and exit\n  --eject     Eject the project using the current configuration\n  --autofix   Autofix supported deprecated warnings by modifying template\n              sources.\n</code></pre> <p>Watch mode</p> <p>When setting up a project, you have to execute <code>ddb configure</code> many times while trying to configure the project  environment.</p> <p>ddb provides a <code>--watch</code> flag to enable Watch mode.</p> <ul> <li><code>ddb --watch configure</code></li> <li><code>ddb -w configure</code></li> </ul> <p>The command will run forever and listen for system file events to perform actions.</p> <p>Use --eject to convert the project to a static version</p> <p><code>--eject</code> option can be used to convert the project to a static version and detach it from ddb.  Templates files are removed after generating destination files.</p> <p>It can be used to distribute sources of your project targeting current configuration only.</p> <p>You may also also set <code>docker.jsonnet.virtualhost_disabled</code> and <code>docker.jsonnet.binary_disabled</code> to <code>True</code> to remove  jsonnet virtualhosts and binaries from generated docker-compose.yml.</p> <pre><code>DDB_OVERRIDE_JSONNET_DOCKER_VIRTUALHOST_DISABLED=1 \\\nDDB_OVERRIDE_JSONNET_DOCKER_BINARY_DISABLED=1 \\\nddb configure --eject\n</code></pre> <p>Deprecated configuration properties and --autofix</p> <p>As <code>ddb</code> evolves during time, some settings and features may become deprecated.</p> <p>When your project use some deprecated configuration property, a warning is displayed like this one.</p> <p>If you are referencing some deprecated configuration keys inside template files, like jsonnet, jinja or ytt, you can run <code>ddb configure --autofix</code> to automatically migrate your template sources. Keep in mind that running this  command will make your project future-proof, but can break things for users running older <code>ddb</code> versions.</p>"},{"location":"commands/#ddb-config","title":"ddb config","text":"<p>Display the effective configuration after merge of all configuration files from possible locations.</p> <pre><code>optional arguments:\n  -h, --help   show this help message and exit\n  --variables  Output as a flat list of variables available in template\n               engines\n  --full       Output full configuration\n  --files      Group by loaded configuration file\n</code></pre> <p>Read more: Configuration</p> <p>Use <code>--variables --full</code> options to check what is available in template engines</p> <p>ddb effective configuration is used as context inside all template engine processing (jinja,  jsonnet, ytt, ...)</p> <p>When working with templates, you might want to include some configuration values into the template.</p> <p>You can use <code>--variables</code> and <code>--full</code> option to display the whole configuration as a flat and dotted notation, as it can be  tedious to retrieve the full variable name from the default <code>yaml</code> output. </p>"},{"location":"commands/#ddb-info","title":"ddb info","text":"<p>Displays human readable informations about your project environment such as environment variables, virtual host,  exposed ports and binaries.</p> <pre><code>optional arguments:\n  -h, --help   show this help message and exit\n  --type TYPE  Filter for a type of information between: bin, env, port and\n               vhost\n</code></pre> <pre><code>+-----------------------------------------------+\n| db                                             |\n+-----------------------------------------------+\n| MYSQL_DATABASE : ddb                          |\n| MYSQL_PASSWORD : ddb                          |\n| MYSQL_ROOT_PASSWORD : ddb                     |\n| MYSQL_USER : ddb                              |\n+-----------------------------------------------+\n| 37306 -&gt; 3306                                 |\n+-----------------------------------------------+\n| mysql                                         |\n| mysqldump                                     |\n+-----------------------------------------------+\n</code></pre> <p>Tip: Use --type to filter for a type of information</p> <p>For instance, you want to see only virtual hosts information. </p> <p>Instead of displaying every section, by writing <code>ddb info --type bin</code> you will be provided with a filtered result. You can choose between bin, env, port and vhost</p> <pre><code>+-----------------------------------------------+\n| db                                            |\n+-----------------------------------------------+\n| Binaries:                                     |\n|                                               |\n| mysql                                         |\n| mysqldump                                     |\n+-----------------------------------------------+\n</code></pre>"},{"location":"commands/#ddb-download","title":"ddb download","text":"<p>Download files from remote sources like Cookiecutter templates.</p> <p>Tip: Use djp packages to build your environment</p> <p>Use published djp packages to build environment from small preconfigured docker compose services.</p>"},{"location":"commands/#ddb-self-update","title":"ddb self-update","text":"<p>If ddb is installed with the standalone binary and a new version is available on github, it will automatically download  it and update the current binary.</p> <pre><code>optional arguments:\n  -h, --help  show this help message and exit\n  --force     Force update\n</code></pre>"},{"location":"commands/#ddb-features","title":"ddb features","text":"<p>This action allows you to check the list of enabled features with a short explanation of what they do. </p> <pre><code>optional arguments:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"commands/#ddb-activate","title":"ddb activate","text":"<p>Display a script for the configured shell that must be evaluated to active the project environment inside the current shell session.</p> <pre><code>optional arguments:\n  -h, --help  show this help message and exit\n  --force     Force activation when a project is already activated.\n</code></pre> <p>Tip: Use $(ddb activate)</p> <p>On bash, you can use <code>$(ddb activate)</code> syntax to activate the project.</p> <p>Read more: Shell feature</p>"},{"location":"commands/#ddb-deactivate","title":"ddb deactivate","text":"<p>Display a script for the configured shell that must be evaluated to deactivate the project environment inside the current shell session.</p> <pre><code>optional arguments:\n  -h, --help  show this help message and exit\n  --force     Force deactivation when a project is already deactivated\n</code></pre> <p>Tip: Use $(ddb deactivate)</p> <p>On bash, you can use <code>$(ddb deactivate)</code> syntax to deactivate the project.</p> <p>Read more: Shell feature</p>"},{"location":"commands/#ddb-check-activated","title":"ddb check-activated","text":"<p>Check if project is activated in current shell.</p> <p>Read more: Shell feature</p>"},{"location":"commands/#ddb-run","title":"ddb run","text":"<p>Display the command line to evaluate in the configured shell to run the registered binary.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>ddb use yaml configuration files:</p> <ul> <li><code>ddb.yml</code>, the main configuration file</li> <li><code>ddb.&lt;env&gt;.yml</code>, the environment configuration file override, where <code>&lt;env&gt;</code> can be <code>dev</code>, <code>stage</code>, <code>ci</code> or <code>prod</code>.</li> <li><code>ddb.local.yml</code>, the configuration file local override.</li> </ul> <p>Those files can be placed in those directories:</p> <ul> <li>Inside <code>~/.docker-devbox/ddb</code></li> <li>Inside <code>~/.docker-devbox</code></li> <li>Inside <code>~</code></li> <li>Inside the project directory root</li> </ul> <p>If many configuration files are found, they are merged with given filenames and directories order.</p> <p>This configuration is used internally by feature actions, but is also injected as context for each supported  template engine. You can add your own project properties inside the configuration file so they are available inside  template engines.</p> <p>How to check effective configuration</p> <p>You can run <code>ddb config</code> to view the effective configuration. Use <code>--full</code> option to display the whole  configuration including default and internal values, and <code>--file</code> to split the configuration settings by  configuration files.</p> <p>Each feature holds it's own configuration section under the name of the feature. For details about supported  configuration settings, please check the documentation of related feature.</p> <p>Default merge behavior</p> <p>By default, when ddb merge a configuration file, objects are be deeply merged, but any other data type is overriden.</p> <p>ddb.yml</p> <pre><code>jsonnet:\n  docker:\n    compose:\n      excluded_services: ['python']\n</code></pre> <p>ddb.local.yml <pre><code>jsonnet:\n  docker:\n    compose:\n      excluded_services: []\n</code></pre></p> <p>effective configuration (ddb config) <pre><code>jsonnet:\n  docker:\n    compose:\n      excluded_services: []\n</code></pre></p> <p>As you can see lists are overriden by default too.</p> <p>Custom merge behavior</p> <p>You can specify custom merge behavior using an object containing two properties</p> <ul> <li><code>value</code>: The actual value to merge</li> <li><code>merge</code>: The merge strategy to apply </li> </ul> <p>For lists, you may use the following merge strategies:</p> <ul> <li><code>override</code> (default)</li> <li><code>append</code></li> <li><code>prepend</code></li> <li><code>insert</code></li> <li><code>append_if_missing</code></li> <li><code>prepend_if_missing</code></li> <li><code>insert_if_missing</code></li> </ul> <p>For objects, you may use the following merge strategies:</p> <ul> <li><code>merge</code> (default)</li> <li><code>override</code></li> </ul> <p>ddb.yml</p> <pre><code>jsonnet:\n  docker:\n    compose:\n      excluded_services: ['python']\n</code></pre> <p>ddb.local.yml <pre><code>jsonnet:\n  docker:\n    compose:\n      excluded_services:\n        merge: append_if_missing\n        value: ['gunicorn']\n</code></pre></p> <p>effective configuration (ddb config) <pre><code>jsonnet:\n  docker:\n    compose:\n      excluded_services: ['python', 'gunicorn']\n</code></pre></p> <p>Configuration and environment variables</p> <p>To override any configuration setting, you may set an environment variable. </p> <p>To convert the property name from dotted notation to environment variable, you should write in UPPERCASE, add  <code>DDB_</code> prefix replace <code>.</code> with <code>_</code>. For instance, <code>core.domain.ext</code> can be overriden with <code>DDB_CORE_DOMAIN_EXT</code>  environment variable.</p> <p>Variables lists can be referenced with <code>[0]</code>, <code>[1]</code>, ..., <code>[n]</code></p> <p>When activating the environment <code>$(ddb activate)</code> with shell feature (Environment activation),  the whole configuration is also exported as environment variables using the same naming convention.</p> <p>Extra configuration files</p> <p>You may add additional configuration files using core.configuration.extra_files configuration property into default  configuration files.</p> <pre><code>core:\n  configuration:\n    extra: ['ddb.custom.yml']\n</code></pre> <p>This will load <code>ddb.custom.yml</code> configuration file from each supported configuration directories. <code>ddb.local.yml</code>  still has the priority other those extra configuration files.</p>"},{"location":"djp/","title":"Djp packages","text":"<p>A djp package (ddb jsonnet package) is a cookiecutter template designed to bring docker-compose  services inside project configuration through jsonnet docker compose library. </p> <p>Services defined inside a djp package are pre-configured to minimize the amount of code to write inside <code>docker-compose.yml.jsonnet</code> configuration file.</p> <p>Many djp packages can be found inside  Github Inetum Orl\u00e9ans organisation with repositories starting with <code>djp-</code>.  They are all documented on their own in <code>README.md</code> file of each repository.</p> <p>djp packages can be configured and downloaded inside the project with  cookiecutter feature and download command.</p> <p>djp packages allow reusability among projects and reduce the complexity of your own  <code>docker-compose.yml.jsonnet</code> configuration.</p> <p>Install <code>inetum-orleans/djp-postgres</code> djp step by step</p> <ul> <li>Register the template inside cookiecutter configuration. <code>extra_context</code> property can be used to customize cookiecutter  template. Available properties and default values can be found in  cookiecutter.json file inside the  repository.</li> </ul> <p>ddb.yml <pre><code>cookiecutter:\n  templates:\n    - template: gh:inetum-orleans/djp-postgres\n      extra_context:\n        postgres_version: \"13\"\n</code></pre></p> <ul> <li>Download cookiecutter templates with the ddb command.</li> </ul> <pre><code>ddb download\n</code></pre> <ul> <li> <p>Check the djp has been downloaded and configured inside <code>.docker/postgres</code>.</p> </li> <li> <p>Import the djp inside <code>docker-compose.yml.jsonnet</code> using <code>ddb.with</code>function. You can read  README.md of the djp repository to check usage. Additional services can still be added manually using jsonnet object composition operator <code>+</code>.</p> </li> </ul> <p>docker-compose.yml.jsonnet</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose(\n    ddb.with(import '.docker/postgres/djp.libjsonnet', name='db') +\n    { services+: {\n        sonarqube: ddb.Image(\"sonarqube:community\")\n            + ddb.VirtualHost(\"9000\", ddb.domain, \"sonarqube\")\n            + {\n                environment: {\n                  'SONAR_ES_BOOTSTRAP_CHECKS_DISABLE': 'true',\n                  'SONAR_JDBC_URL': 'jdbc:postgresql://db:5432/' + ddb.projectName,\n                  'SONAR_JDBC_USERNAME': ddb.projectName,\n                  'SONAR_JDBC_PASSWORD': ddb.projectName\n                },\n                volumes: [\n                    \"sonarqube-db-data:/var/lib/postgresql/data:rw\",\n                    \"sonarqube_conf:/opt/sonarqube/conf\",\n                    \"sonarqube_data:/opt/sonarqube/data\",\n                    \"sonarqube_extensions:/opt/sonarqube/extensions\",\n                    \"sonarqube_bundled-plugins:/opt/sonarqube/lib/bundled-plugins\",\n                    ddb.path.project + \"/.docker/sonarqube/sonar.properties:/opt/sonarqube/conf/sonar.properties\",\n                    ddb.path.project + \"/plugins/sonarqube-community-branch-plugin.jar:/opt/sonarqube/extensions/plugins/sonarqube-community-branch-plugin.jar\",\n                    ddb.path.project + \"/plugins/sonarqube-community-branch-plugin.jar:/opt/sonarqube/lib/common/sonarqube-community-branch-plugin.jar\",\n                    ddb.path.project + \"/plugins/sonar-dependency-check-plugin.jar:/opt/sonarqube/extensions/plugins/sonar-dependency-check-plugin.jar\",\n                    ddb.path.project + \"/plugins/sonar-auth-oidc-plugin.jar:/opt/sonarqube/extensions/plugins/sonar-auth-oidc-plugin.jar\"\n                ]\n            }\n        }\n    }\n)\n</code></pre> <p>Advanced <code>docker-compose.yml.jsonnet</code> using djp packages / ddb.with() usage</p> <p><code>ddb.with()</code> accepts some optional arguments:</p> <ul> <li><code>name</code>: Name of the service</li> <li><code>params</code>: Parameters of the service</li> <li><code>append</code>: Additional configuration to add</li> <li><code>when</code>: Condition to add the service</li> </ul> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose(\n    ddb.with(import '.docker/postgres/djp.libjsonnet',\n    name='db',\n    params={database: 'estamp'},\n    append={\n        volumes+: [\n            ddb.path.project + \"/.docker/postgres/data.sql:/docker-entrypoint-initdb.d/11-data.sql\",\n            ddb.path.project + \"/.docker/postgres/data.fix-jpa-datatypes.sql:/docker-entrypoint-initdb.d/21-data.fix-jpa-datatypes.sql\"\n        ]\n    }) +\n\n    ddb.with(import '.docker/mailcatcher/djp.libjsonnet',\n    when=!ddb.env.is('prod')) +\n\n    ddb.with(import '.docker/openjdk/djp.libjsonnet') +\n\n    ddb.with(import '.docker/maven/djp.libjsonnet') +\n\n    ddb.with(import '.docker/apache/djp.libjsonnet',\n    name='web',\n    params={domain: domain}) +\n\n    ddb.with(import '.docker/openjdk/djp.libjsonnet',\n    name='api',\n    append=ddb.VirtualHost(\"8080\", std.join('.', [\"api\", domain]), \"api\") + {\n        tty: false,\n        environment+: [\n         \"SPRING_PROFILES_ACTIVE=\" + std.extVar(\"core.env.current\")\n        ],\n        working_dir: \"/project/runtime\",\n        entrypoint: \"java\",\n        command: \"-jar ../target/estamp-1.0-SNAPSHOT.jar\",\n    })\n)\n</code></pre> <p>Content of a <code>djp</code> package</p> <p>A <code>djp</code> package contains a <code>djp.libjsonnet</code> file that exports a jsonnet object. Rest of the  package content are files that should be available in the docker context or used by docker-compose  generated configuration.</p> <p>The jsonnet object should match the following structure:</p> Property Description <code>factory(name, params={})</code> Function called by <code>ddb.with</code> that returns a part of docker compose configuration using given service <code>name</code> and <code>params</code>. Available parameters are documented in README.md file of the djp git repository. <code>defaultName</code> Default name generated by <code>ddb.with</code> function. Should match the cookiecuttter directory. <p>Publish a <code>djp</code> package</p> <p>Your can publish a <code>djp</code> package by creating a new public github repository named with <code>djp-*</code> from  inetum-orleans/djp-template template. This is a github  template that contains a basic structure common to any <code>djp</code> package.</p> <p>After creating the repository, you should follow those steps to make sure the djp will be usable.</p> <ul> <li> <p>In <code>README.md</code>, customize the title djp-template, Description section, Snippet section and Parameters section.  Usage section should be left untouched.</p> </li> <li> <p>In <code>cookiecutter.json</code>, add the settings allong with default values of the djp. Those settings are then available at download time inside <code>cookiecutter.templates.extra_context</code> property  (see cookiecutter Feature).</p> </li> <li> <p>Everything is processed as a template inside cookiecutter. You can reference properties defined in  <code>cookiecutter.json</code> using <code>{{ cookiecutter.&lt;property&gt; }}</code> Jinja2 syntax.</p> </li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":"How to customize the configuration for a single project ? <p>You may create a <code>ddb.local.yml</code> configuration file in the project directory.</p> <p>Read more: Configuration</p> How to customize the configuration for all project on the host ? <p>You may create a <code>ddb.yml</code> configuration file in the <code>~/.docker-devbox</code> installation directory or in your home folder.</p> <p>You can also add environment variable matching the property name to override, in UPPERCASE, prefixed  with <code>DDB_</code> and <code>.</code> replaced with <code>_</code>.</p> <p>Read more: Configuration</p> How to change the domain name for a single project ? <p>Domain name is configured through 2 <code>core</code> settings.</p> <pre><code>core.domain.ext: test\ncore.domain.sub: folder\n</code></pre> <p>Those settings are joined with <code>.</code> to build the main domain name (<code>folder.test</code>).</p> <p>Read more: Configuration</p> How to change the domain extension for all projects ? <p>Domain name is configured through 2 <code>core</code> settings.</p> <pre><code>core.domain.ext: test\ncore.domain.sub: folder\n</code></pre> <p>Those settings are joined with <code>.</code>, <code>folder.test</code> to build the main domain name.</p> <p>You can override <code>core.domain.ext</code> setting globally by creating a <code>ddb.local.yml</code> file in <code>~/.docker-devbox</code> installation directory or in your home folder.</p> <p>You can also define a system environment variable named <code>DDB_CORE_DOMAIN_EXT</code> with the domain extension.</p> <p>Read more: Configuration</p> How to disable/enable tags from docker images defined in generated docker-compose.yml file ? <p>In your project <code>ddb.yml</code> file, you may disable this option in the jsonnet feature  with <code>jsonnet.docker.build.image_tag_from</code> set to false.</p> <p>Read more: Configuration</p> CI fails with the following message: \"the input device is not a TTY\" <p>If no TTY is available, you have to set the following environment variable to workaround this issue</p> <p>COMPOSE_INTERACTIVE_NO_CLI=1</p> How to clear cache ? <p>Run <code>ddb --clear-cache configure</code> or <code>rm -Rf ~/.docker-devbox/cache</code>.</p> ddb fails to run with message: <code>version 'GLIBC_2.35' not found</code> <p>It seems you are running an old, unsupported linux distribution, like Ubuntu 16.04.</p> <p>Currently, ddb works on ubuntu 22.04 or equivalent, using glibc 2.35 or higher. You may also have luck installing ddb from pip if your distribution can run a supported python version.</p> What to do in case of a <code>ddb self-update</code> failure ? <p>Starting from version 3.1.0, ddb creates a <code>ddb.old</code> backup file in the installation directory (usually <code>~/.docker-devbox/bin</code>), before upgrading to a higher version.</p> <p>You can restore it by running the following command:</p> <pre><code>mv ~/.docker-devbox/bin/ddb.old ~/.docker-devbox/bin/ddb\n</code></pre> <p>Otherwise you may also install a specific version of ddb by using the command below:</p> <pre><code>curl -L -o ~/.docker-devbox/bin/ddb https://github.com/inetum-orleans/docker-devbox-ddb/releases/download/v3.0.2/ddb-linux &amp;&amp; chmod +x ~/.docker-devbox/bin/ddb\n</code></pre> <p>You may also download one of the release from the github release page manually and place it in the installation directory.</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#technology-agnostic","title":"Technology agnostic","text":"<p>ddb is not tied to a particular language, framework or technical stack.</p> <p>It's designed to automate application and docker-compose configuration on an environment, so you forget  about the docker hard stuff while writing code, and so application can be deployed to stage and production  environment with no changes on your sources.</p>"},{"location":"overview/#docker-devbox","title":"Docker Devbox","text":"<p>Even if ddb can be used as a standalone tool, it has been designed with docker and  docker-compose in mind. </p> <p>Many features requires some companion containers to run in background. Those containers have been configured and  packaged in docker-devbox, so you should really  consider installing it.</p> <p>docker-devbox brings the following containers:</p> <ul> <li>traefik, to automatically proxy services provided by docker-compose using project domain name virtualhost on HTTP/80 and HTTPS/443, and generate SSL certificates with Letsencrypt (for public domain names).</li> <li>cfssl, the Cloudflare's PKI and TLS toolkit, to generate certificates for internal domains.</li> <li>portainer to manage containers from a web browser.</li> </ul> <p>Please read docker-devbox README to perform  the installation properly.</p> <p>Eat your own dog food</p> <p>docker-devbox containers are configured with ddb themselves.</p>"},{"location":"overview/#features-actions-and-events","title":"Features, actions and events","text":"<p>ddb make use of features to perform it's magic.</p> <p>Each feature holds it's own configuration and embeds one or many actions. An action configures bindings between an event and a function, so when an event occurs, each binded functions are called.</p> <p>An action's function implementation can also raise other events that will trigger other feature action functions,  making ddb a reactive software.</p> <p>What occurs when running <code>ddb configure</code> ?</p> <p>configure is the most import command in ddb.</p> <p>When running <code>ddb configure</code> command, a bunch of events are triggered.</p> <p>Firstly, the command raise the <code>phase.configure</code> event, which is binded to the file feature.</p> <p>Secondly, file feature scans files and triggers <code>file:found</code> event for each file in the  project directory.</p> <p>Thirdly, the jsonnet feature, which is binded to <code>file:found</code> event with a filter to match <code>.jsonnet</code> file extension only, so that only those files are processed by the jsonnet template engine.</p> <p>Other features perform actions the same way, like jinja, symlinks  and ytt features.</p> <p>Those actions raises other events, like <code>file:generated</code> for each generated files, that will be processed by the  gitignore feature to add generated files to ignore list.  </p> <p>And so on.</p>"},{"location":"templates/","title":"Templates","text":"<p>ddb provides some template engine support, like Jinja,  Jsonnet and ytt.</p> <p>Each template engine is implemented in it's own feature, jinja,  jsonnet and ytt.</p> <p>Files are automatically processed</p> <p>Files are automatically processed by a template engine based on file extension.</p> <p><code>Dockerfile.jinja</code> is processed by jinja and produces <code>Dockerfile</code> file.</p> <p>The template filename extension can also be found when placed just before the target filename extension.</p> <p>Both <code>data.json.jinja</code> and <code>data.jinja.json</code> is processed by jinja and produces <code>data.json</code>.</p> <p>Automate your deployments for all environments</p> <p>Templates are perfect for configuration files that should change for each environment, or to embed some global  configuration settings into many static configuration files.</p> <p>You should abuse of templates to make deployment on various environment a breeze, as the application will  auto-configure based on ddb configuration of each environment.</p> <p><code>git clone</code>, <code>ddb configure</code> and that's all ! Your application is ready to run on dev, stage or production  environment.</p> <p>Keep in mind that you can freely add any setting inside <code>ddb.yml</code> project configuration, it   so you can retrieve them from any template.</p>"},{"location":"templates/#jinja","title":"Jinja","text":"<p>Jinja is a modern and designer-friendly templating language, modelled after Django\u2019s templates. It's a general purpose  templating engine that doesn't target any particular file format.</p> <p>Write <code>Dockerfile.jinja</code> instead of raw <code>Dockerfile</code></p> <p>Jinja is really handy to make your Dockerfile dynamic.</p> <p>Writing <code>Dockerfile.jinja</code> files instead of raw <code>Dockerfile</code> is recommended in ddb, and mandatory when using fixuid  feature.</p>"},{"location":"templates/#jsonnet","title":"Jsonnet","text":"<p>Jsonnet is a data templating language for app and tool developers. It's an extension of json  and can only output <code>JSON</code>.</p> <p>In ddb, Jsonnet mostly works like others template engines, but the extension of the target file is used to guess the  target output format: <code>JSON</code> or <code>YAML</code>. As Jsonnet can only output <code>JSON</code>, <code>YAML</code> output is converted from <code>JSON</code>  thanks to python environment.</p> <ul> <li> <p><code>docker-compose.yml.jsonnet</code> is processed by jsonnet and produces a yaml as output.</p> </li> <li> <p><code>data.jsonnet.json</code> is processed by jsonnet and produces json as output.</p> </li> </ul> <p>Jsonnet produces <code>json</code> as default output</p> <p>By default, jsonnet produces json as output, unless extension of the target file is <code>.yml</code>.</p> <p>Write <code>docker-compose.yml.jsonnet</code> instead of raw <code>docker-compose.yml</code></p> <p>ddb brings many docker and docker-compose features through jsonnet, so you'd better writing a  <code>docker-compose.yml.jsonnet</code> file instead of a raw <code>docker-compose.yml</code> configuration.</p> <p>Read more: jsonnet Feature (Docker-compose jsonnet library)</p>"},{"location":"templates/#ytt","title":"ytt","text":"<p>ytt is a templating tool that understands YAML structure allowing you to focus on your data  instead of how to properly escape it.</p> <p>Ytt templates are a superset of YAML and can only generate yaml files.</p> <p>You may like ytt, but you'd better focus on learning Jsonnet</p> <p>Because Jsonnet supports custom functions, and both <code>JSON</code> et <code>YAML</code> output, we consider  ytt as a secondary template engine. </p> <p>It's available and fully supported in ddb, so you can use it to template any <code>YAML</code> configuration files inside your application.</p> <p>But if you implement <code>docker-compose.yml.ytt</code>, you won't be able to use all features available in ddb docker-compose jsonnet library.</p> <p>Read more: jsonnet Feature (Docker-compose jsonnet library)</p>"},{"location":"templates/#symlinks","title":"Symlinks","text":"<p>Symlinks feature allow to create a symlink from many possible files, each source file matching  a supported environment.</p> <p>Even if it's not based on a template engine, symlinks feature shares some of behavior from  other template based features as it generates a symlink from another file.</p> <p>By default, ddb holds the following configuration settings:</p> <pre><code>core.env.available: ['prod', 'stage', 'ci', 'dev']\ncore.env.current: dev\n</code></pre> <ul> <li><code>core.env.available</code> contains all supported environment values</li> <li><code>core.env.current</code> match the actual environment.</li> </ul> <p>Consider a project with a configuration file named <code>settings.yaml</code>. This file should be different for each environment.</p> <p>Thanks to ddb and without implementing custom configuration logic inside the application, you can create a file for each environment, and the symlink matching the current environment is generated.</p> <ul> <li><code>settings.yaml.prod</code> -&gt; Settings for prod environment</li> <li><code>settings.yaml.stage</code> -&gt; Settings for stage environment</li> <li><code>settings.yaml.dev</code> -&gt; Settings for dev environment</li> <li><code>settings.yaml</code> -&gt; Symlink pointing to file based on <code>core.env.current</code></li> </ul> <p>If <code>core.env.current</code> is set to <code>dev</code>, <code>settings.yaml</code> symlink points to <code>settings.yaml.dev</code></p> <p>If <code>core.env.current</code> is set to <code>stage</code>, <code>settings.yaml</code> symlink points to <code>settings.yaml.stage</code></p> <p>If <code>core.env.current</code> is set to <code>prod</code>, <code>settings.yaml</code> symlink points to <code>settings.yaml.prod</code></p> <p>What if no file exists for the current environment ?</p> <p>There's no <code>settings.yaml.ci</code> file, but if <code>core.env.current</code> is set to <code>ci</code>, <code>settings.yaml</code> symlink will still point  to <code>settings.yaml.dev</code></p> <p>The fallback behavoir is to find the first existing file on the right of <code>ci</code> in <code>core.env.available</code>.</p> <p>If <code>settings.yaml.dev</code> doesn't exists, no symlink is created at all.</p> <p>So now, you can refer to this symlink to load the configuration inside your application, so it will switch  automatically when deploying on <code>stage</code> or <code>prod</code> environment.</p> <p>Where to use a symlink, where to use a template ?</p> <p>Use a symlink where changes are conditionned by <code>core.env.current</code> environment setting and affects most of the  configuration file content. </p> <p>Use a template where changes are conditionned by any other environnement setting, or when changes affects a very  small portion of the configuration file content.</p> <p>Anyway, You can use both symlink and template to generate a single configuration file as ddb is reactive and  supports natural action chaining. </p> <ul> <li> <p>You can create a symlink first, and then resolve a template.</p> <ul> <li><code>settings.yaml.ytt.prod</code> -&gt; Settings template for prod environment</li> <li><code>settings.yaml.ytt.stage</code> -&gt; Settings template for stage environment</li> <li><code>settings.yaml.ytt.dev</code> -&gt; Settings template for dev environment</li> <li><code>settings.yaml.ytt</code> -&gt; Symlink pointing to settings template file based on <code>core.env.current</code></li> <li><code>settings.yaml</code> -&gt; Generated settings file from Symlink through ytt template engine</li> </ul> </li> <li> <p>Or you can resolve a template first, and then create a symlink.</p> <ul> <li><code>settings.yaml.prod.ytt</code> -&gt; Settings template for prod environment</li> <li><code>settings.yaml.stage.ytt</code> -&gt; Settings template for stage environment</li> <li><code>settings.yaml.dev.ytt</code> -&gt; Settings template for dev environment</li> <li><code>settings.yaml.prod</code> -&gt; Generated settings file for prod environment</li> <li><code>settings.yaml.stage</code> -&gt; Generated settings file for stage environment</li> <li><code>settings.yaml.dev</code> -&gt; Generated settings file for dev environment</li> <li><code>settings.yaml</code> -&gt; Symlink pointing to generated settings file based on <code>core.env.current</code></li> </ul> </li> </ul>"},{"location":"features/certs/","title":"Certs","text":"<p>For some project, we needed to have SSL activated for HTTPS access of our projects. So, SSL certificates became a project dependency.</p> <p>The issue was that in local environments, sometimes we cannot open port 80 to the world and get a true Let's Encrypt certificate or don't want to use the native traefik certificate management.</p> <p>One solution we found is to manage certificates using CFSSL to generate and manage a true certificate generation locally. This feature currently handle only CFSSL certificate generation and management.</p> <p>Feature configuration (prefixed with <code>certs.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>type</code> string<code>cfssl</code> Type of certificate generation. Currently, only <code>cfssl</code> is supported. <code>cfssl.server.host</code> boolean<code>localhost</code> CFSSL host (without protocol). <code>cfssl.server.port</code> integer<code>7780</code> CFSSL port to connect to. <code>cfssl.server.ssl</code> integer<code>false</code> Should SSL be used to connect ? <code>cfssl.server.verify_cert</code> integer<code>false</code> Should the CFSSL SSL certificate be verified on connect ? Advanced Property Type Description <code>cfssl.verify_checksum</code> integer<code>false</code> Should the CFSSL generated certificates be verified with checksums. <code>cfssl.append_ca_certificate</code> boolean<code>true</code> Should the signer (CA) certificate be appended to the generated certificate ? <code>destination</code> string<code>.certs</code> Destination directory of generated certificates. <code>signer_destinations</code> string[]<code>[]</code> Additional destinations for signer (CA) certificates. Internal Property Type Description <code>cfssl.writer.filenames.certificate</code> string<code>%s.crt</code> Filename template of generated PEM certificates. <code>cfssl.writer.filenames.certificate_der</code> string<code>%s.crt.der</code> Filename template of generated DER certificates. <code>cfssl.writer.filenames.certificate_request</code> string<code>%s.csr</code> Filename template of PEM generated certificate requests. <code>cfssl.writer.filenames.certificate_request_der</code> string<code>%s.csr.der</code> Filename template of name of DER geberated certificate requests. <code>cfssl.writer.filenames.private_key</code> string<code>%s.key</code> Filename template of generated PEM private key."},{"location":"features/certs/#certificate-generation","title":"Certificate generation","text":"<p>When running <code>ddb configure</code> command, this action will be triggered. Based on the configuration, the right certificate manager will be used to generate the appropriate certificate in the <code>certs.destination</code> folder if it does not already exist.</p> <p>At the end, it will trigger a <code>certs.generated</code> and <code>certs.available</code> event which can be used by other features to use those generated certificates.</p> <p>For instance, the traefik is listening for the <code>certs.available</code> event in order to update the configuration in order to use them for HTTPS.</p>"},{"location":"features/certs/#certificate-removal","title":"Certificate removal","text":"<p>When running <code>ddb configure</code> command, this action will be triggered. Based on the configuration, if certificates was generated before and the <code>certs.type</code> is switch to something else than <code>cfssl</code>, those certificates will be deleted.</p> <p>At the end, it will trigger a <code>certs.removed</code> event which can be used by other features to update themselves.</p> <p>For instance, the traefik is listening for the <code>certs.removed</code> event in order to update the configuration in order to remove them. </p>"},{"location":"features/cookiecutter/","title":"Cookiecutter","text":"<p>Download and generate files from Cookiecutter templates.</p> <p>This can be used for djp packages or any other Cookiecutter with  download command.</p> <p>Feature configuration (prefixed with <code>cookiecutter.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>templates</code> Template[] List of templates to download Advanced Property Type Description <code>default_context</code> Dict[str, Any] List of available environments. You should any new custom environment to support here before trying to set <code>env.current</code> to this custom environment. Internal Property Type Description <code>cookiecutters_dir</code> string Temporary directory for download templates <code>replay_dir</code> string Temporary directory to store replay <p>Template configuration (used in <code>cookiecutter.templates</code>)</p> Simple Property Type Description <code>template</code> string* Template to download <code>output_dir</code> string Output directory relative to project root. If not defined, it will use <code>directory</code> value defined in template's <code>cookiecutter.json</code> <code>checkout</code> string tag/branch to use for the template <code>extra_context</code> dict[str, Any] Extra context to use to render cookiecutter template. This can be used to customize values from template's <code>cookiecutter.json</code> Advanced Property Type Description <code>password</code> dict[str, Any] Password to use for authentication <code>no_input</code> boolean<code>true</code> Set to <code>false</code> to make cookiecutter ask questions. <code>replay</code> boolean<code>false</code> <code>overwrite_if_exists</code> boolean<code>true</code> <code>config_file</code> string <code>default_config</code> dict[str, Any] <p>Generate <code>.patch</code> files to track your changes</p> <p>If you need to apply some modification on downloaded content, you should generate <code>.patch</code> files (unified diff)  of the output directory of the template.</p> <p>Any <code>.patch</code> file available in the output directory will be automatically applied when invoking <code>ddb download</code>  command later, so you changes are kept while downloading a new version of the template.</p>"},{"location":"features/copy/","title":"Copy","text":"<p>The <code>copy</code> feature is a simple way to automatically copy files from one folder to another. </p> <p>This feature is bounded to the <code>init</code> command.</p> <p>Feature configuration (prefixed with <code>copy.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>specs</code> Spec[] List of specifications of files/patterns to copy <p>Spec configuration (used in <code>copy.specs</code>)</p> Property Type Description <code>source</code> string* The source file to copy, or a glob expression matching files to copy. it can be a local file path, or it can starts with <code>http(s)://</code> to copy from a remote URL. <code>destination</code> string<code>.</code> The exact target destination file or directory. <code>filename</code> string The destination filename. If empty and <code>destination</code> match a directory, <code>source</code> filename will be used. <code>dispatch</code> string[] A list of directories or directory globs where the file will be duplicated. i.e if set to <code>['target']</code>, source file with be copied to <code>target</code> directory using filename defined in <code>destination</code> property. If set to ['target/*<code>], it will be copied in each subdirectory of target directory using filename defined in</code>destination` property. <p>Copy a file from an URL</p> <pre><code>copy:\n  specs:\n    - source: 'https://github.com/boxboat/fixuid/releases/download/v0.5/fixuid-0.5-linux-amd64.tar.gz'\n      destination: '.docker'\n      filename: 'fixuid.tar.gz'\n</code></pre> <p>Copy many files from filesystem</p> <pre><code>copy:\n  specs: \n    - source: '/etc/ssl/certs/*'\n      destination: 'host-certs'\n</code></pre>"},{"location":"features/core/","title":"Core","text":"<p>Core feature contains some key configuration like domain name and current active environement. Those configuration  settings may impact many other features indirectly.</p> <p>It also handle the two following basic commands : <code>ddb features</code> and <code>ddb config</code>. Check command page for details about those commands.</p> <p>Feature configuration (prefixed with <code>core.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>domain.sub</code> string<code>${project.name}</code> The domain to use for the domain generation. This is the constant part of the domain, that should not vary between environment. <code>domain.ext</code> string<code>test</code> The extension to use for the domain. This is the last part, of your domain name, that may vary between environment. <code>domain.value</code>(read only) string<code>${project.name}.${domain.ext}</code> The whole domain name. <code>env.current</code> string<code>${env.available}[-1]</code> Current active environment. Default value is <code>dev</code>, or the last value of <code>env.available</code>. <code>project.name</code> string<code>&lt;Directory name of ${path.project_home}&gt;</code> The project name. This is used by many templates and to generate other default values like <code>domain.sub</code>. Advanced Property Type Description <code>env.available</code> string[]<code>['prod', 'stage', 'ci', 'dev']</code> List of available environments. You should any new custom environment to support here before trying to set <code>env.current</code> to this custom environment. <code>required_version</code> string Minimal required <code>ddb</code> version for the project to work properly. If <code>required_version</code> is greater than the currently running one, ddb will refuse to run until it's updated. <code>check_updates</code> boolean<code>true</code> Should check for ddb updates be enabled ? <code>release_asset_name</code> string<code>&lt;plaform dependent&gt;</code> Github release asset name to use to download ddb on <code>self-update</code> command. <code>path.ddb_home</code> string<code>${env:HOME}/.docker-devbox/ddb</code> The path where ddb is installed. <code>path.home</code> string<code>${env:HOME}/.docker-devbox</code> The path where docker devbox is installed. <code>path.user_home</code> string<code>${env:HOME}</code> The user home path <code>path.project_home</code> string The project directory. <code>process</code> Process[] List of process configurations. A process configuration allow to add custom arguments before and after a command executed internally (like <code>git</code>). Internal Property Type Description <code>os</code> string<code>posix</code> The current operating system. <code>github_repository</code> string<code>inetum-orleans/docker-devbox-ddb</code> List of process configurations. A process configuration allow to add flags before and after a command normaly runned by ddb (like <code>git</code>). <p>Process configuration (used in <code>core.process</code>)</p> Property Type Description <code>bin</code> string* The process path to override. <code>prepend</code> string|string[] Arguments to prepend to default arguments. <code>append</code> string|string[] Arguments to append to default arguments."},{"location":"features/docker/","title":"Docker","text":"<p>The docker feature brings docker and docker-compose integration. You may also check Jsonnet Feature, as using jsonnet docker specific library brings many Docker related features to <code>ddb</code>.</p> <p>Feature configuration (prefixed with <code>symlinks.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>ip</code> string IP Address of the docker engine <code>interface</code> string Network interface of the docker engine <code>docker_command</code> string<code>docker</code> command used to run docker CLI <code>docker_compose_command</code> string<code>docker compose</code> command used to run docker compose CLI <code>user.uid</code> string The user UID to use inside a container when jsonnet <code>User()</code> function is used. <code>user.gid</code> string The user GID to use inside a container when jsonnet <code>User()</code> function is used. <code>user.name</code> string The host username that will get converted to UID to use inside a container when jsonnet <code>User()</code> function is used. <code>user.group</code> string The host groupname that will get converted to GID to use inside a container when jsonnet <code>User()</code> function is used. <code>path_mapping</code> Dict[str, str] Path mappings to apply on declared volume sources."},{"location":"features/docker/#docker-compose-configuration-processing","title":"docker-compose configuration processing","text":"<p>When a <code>docker-compose.yml</code> is found or generated from templates, the content is parsed.</p> <p>All labels prefixed <code>ddb.emit.</code> are processed and converted into event and event arguments.</p> <p>Creation of binaries</p> <p>Whether you use <code>ddb.Binary()</code> in jsonnet template or manually add labels to <code>docker-compose.yml</code>, they  are converted into ddb configuration and shims are generated to run the declared binary as simple executable  command, thanks to shell feature.</p>"},{"location":"features/file/","title":"File","text":"<p>As ddb work on files, there is a dedicated feature which soul purpose is to walk through files from the project directory and trigger events when files are found or removed.</p> <p>Feature configuration (prefixed with <code>jinja.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>extensions</code> string[] A list of glob of supported extension. <code>includes</code> string[] A list of glob of directories to include. <code>excludes</code> string[]<code>['**/_*', '**/.git', '**/.idea', '**/node_modules', '**/vendor', '**/target', '**/dist']</code> A list of glob of directories to exclude. <code>include_files</code> string[] A list of glob of files to include. <code>exclude_files</code> string[] A list of glob of files to exclude."},{"location":"features/file/#file-walk-and-event-triggering","title":"File walk and event triggering","text":"<p>As previously said, the soul purpose of the feature is to walk through files from the project directory, recursively.</p> <p>It does not dive into folders set in <code>file.excludes</code> configuration.  For instance, the directory <code>node_modules</code> which contains npm installed modules on the project is not process. By doing so, the feature is faster.</p> <p>When it checks for a folder, each file not excluded is put into a <code>file:found</code> event which will be caught by other  features such as jsonnet or docker feature, and store those file in ddb cache.</p> <p>For next executions, using this cache, the feature will detect if a file have been deleted and raise a <code>file.deleted</code>. This event is used by the gitignore feature to remove the file from the gitignore if needed.</p> <p>The watch mode</p> <p>This feature is the one that will benefit the most from the watch mode described in the command  section.</p> <p>It will then be kept active and check if a file is created, modified, moved or even deleted.</p>"},{"location":"features/fixuid/","title":"Fixuid","text":"<p>One common pitfall when working with Docker is file permission management of mounted volumes.</p> <p>Those permission issues are related to the way docker works and cannot really be fixed once for all.</p> <p>To help developer fixing permission issues, fixuid is auto-configured by ddb when a <code>fixuid.yml</code> file is available in docker build context.</p> <p>Feature configuration (prefixed with <code>fixuid.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? Internal Property Type Description <code>url</code> string<code>https://github.com/boxboat/fixuid/releases/download/v0.5/fixuid-0.5-linux-amd64.tar.gz</code> URL to download the fixuid distribution binary."},{"location":"features/fixuid/#automatic-configuration","title":"Automatic configuration","text":"<p>In order to benefit from this feature, few steps are require. </p> <p>First, you need to create the <code>fixuid.yml</code> configuration file next to <code>Dockerfile.jinja</code>. For this feature to work properly, you must use a template feature for your Dockerfile, like Jinja. </p> <p>In this <code>fixuid.yml</code> configuration file, you have to define three settings: </p> <ul> <li><code>user</code>: the user inside the container which is allowed to run command and access files.</li> <li><code>group</code>: the group inside the container which is allowed to run command and access files.</li> <li><code>paths</code>: the list of paths inside the container where permissions will be fixed. Volumes mount point must also be listed.</li> </ul> <p>For more details on this configuration, please refer to the  fixuid documentation.</p> <p>Example : A posgresql fixuid configuration</p> <pre><code>user: postgres\ngroup: postgres\npaths:\n  - /\n  - /var/lib/postgresql/data\n</code></pre> <p>Then, in your <code>docker-compose.yml.jsonnet</code>, you should use ddb.User() in order to map your local  user to the container. </p> <p>Finally, run the <code>ddb configure</code> command to generate <code>Dockerfile</code>. Instructions related to fixuid should have  been generated. The entrypoint is changed to run fixuid before the default entrypoint.</p> <p>Example with PostgreSQL</p> <pre><code>FROM postgres\n\n# Mount this volume to help loading/exporting dumps\nRUN mkdir /workdir\nVOLUME /workdir\n\nUSER postgres\n</code></pre> <p>Generates the following when <code>fixuid.yml</code> file is available in the <code>Dockerfile</code> directory.</p> <pre><code>FROM postgres\n\n# Mount this volume to help loading/exporting dumps\nRUN mkdir /workdir\nVOLUME /workdir\n\nUSER postgres\n\nADD fixuid.tar.gz /usr/local/bin\nRUN chown root:root /usr/local/bin/fixuid &amp;&amp; chmod 4755 /usr/local/bin/fixuid &amp;&amp; mkdir -p /etc/fixuid\nCOPY fixuid.yml /etc/fixuid/config.yml\nUSER postgres\nENTRYPOINT [\"fixuid\", \"-q\", \"docker-entrypoint.sh\"]\nCMD [\"postgres\"]\n</code></pre> <p>With this configuration, you should be able to generate a dump from the container as your own user instead of root.</p> <p>Why should I use <code>.jinja</code> extension for my <code>Dockerfile</code> ?</p> <p>You should always use <code>.jinja</code> when declaring a Dockerfile to benefits of <code>fixuid</code> feature.    </p> <p>Using <code>fixuid</code> feature updates <code>Dockerfile</code> to add the instructions needed for fixuid. That's why you should  use a template source instead, for this to be generated again on each <code>ddb</code> configure command. </p> <p>The <code>Dockerfile</code> will be automatically ignored by git thanks to gitignore feature.</p> <p>If you are not using <code>jsonnet</code> for <code>docker-compose.yml</code></p> <p>In <code>docker-compose.yml</code>, you will need to add the configuration <code>user</code> to the service using this  docker container.</p> <p>You will need to set it manually with the <code>uid</code> and <code>gid</code> of the host user which will run the container and execute  commands.</p> <p>As many other ddb features are available through jsonnet feature, you should really consider using  it. This will help you to build simpler, shorter and smarter <code>docker-compose</code> configuration.</p> <p>Disable or customize fixuid automatic configuration</p> <p>You may need to disable fixuid automatic configuration. Use the following comments in your Dockerfile.jinja.</p> <ul> <li><code># fixuid-disable</code>: Disable both download of fixuid.tar.gz and whole auto-configuration. Use this to disable fixuid totally for this Dockerfile.</li> <li><code># fixuid-manual</code>: Only keep download of fixuid.tar.gz. Use this to configure fixuid totally manually in the Dockerfile.</li> <li><code># fixuid-manual-install</code>: Keep download of fixuid.tar.gz and auto-configuration of ENTRYPOINT. You still have to install fixuid manually in the Dockerfile.</li> <li><code># fixuid-manual-entrypoint</code>: Keep download of fixuid.tar.gz and installation of required files. You still have to invoke fixuid manually in the Entrypoint.</li> </ul>"},{"location":"features/git/","title":"Git","text":"<p>The Git feature provides automation relatives to the git configuration of the project.</p> <p>Feature configuration (prefixed with <code>git.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>fix_files_permissions</code> boolean<code>true</code> Should file permissions be fixed from git index (executable flags) ? <p>Configuration</p> <pre><code>git:\n  disabled: false\n  fix_files_permissions: true\n</code></pre>"},{"location":"features/git/#fix-files-permissions","title":"Fix files permissions","text":"<p>When you clone or update a repository, it may contain executable files. If you are working on windows and using some  synchronisation too to synchronize those files, executable flags may be lost.</p> <p>With Git, you can store the executable flag into the repository. To do so, you can execute the following command : </p> <pre><code>git update-index --chmod=+x &lt;your_file&gt;\n</code></pre> <p>But this will not update the flag on the system automatically.</p> <p>Unless <code>git.fix_files_permissions</code> is set to <code>false</code> in ddb configuration, files marked as executable in git repository have their permissions fixed on <code>ddb configure</code> command. </p>"},{"location":"features/gitignore/","title":"Gitignore","text":"<p>The Gitignore feature provides automation to the management of .gitignore files of the project.</p> <p>Feature configuration (prefixed with <code>gitignore.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>enforce</code> string[]<code>['*ddb.local.*']</code> List of file globs to force into <code>.gitignore</code>."},{"location":"features/gitignore/#automatic-management-of-gitignore","title":"Automatic management of gitignore","text":"<p>ddb generates a bunch of files inside your project sources: target files from templates, binary shims, ...</p> <p>As those files may depends on your environment configuration, they should not be added to the git repository and must  be added to <code>.gitignore</code>, for them to be generated on each environment without spoiling git repository.</p> <p>Because adding them manually to <code>.gitignore</code> is a chore, ddb automatically adds all generated files to the nearest  <code>.gitignore</code> file, from top to bottom of filesystem folder hierarchy. The other way round, if a generated file source  is removed, the target file will also be removed from <code>.gitignore</code> file.</p> <p>Finally, using the <code>enforce</code> configuration, you can force files to be added to the gitignore, even if it is not a file  managed by ddb.</p>"},{"location":"features/jinja/","title":"Jinja","text":"<p>Jinja is template library included in <code>ddb</code>. It is used to generate files using ddb configuration.</p> <p>Feature configuration (prefixed with <code>jinja.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>suffixes</code> string[]<code>['.jinja']</code> A list of filename suffix to include. <code>extensions</code> string[]<code>['.*', '']</code> A list of glob of supported extension. <code>excludes</code> string[]<code>[]</code> A list of glob of filepath to exclude. <code>options</code> dict[string, object] Additional options to pass to Jinja Environment. Advanced Property Type Description <code>includes</code> string[]<code>['*.jinja{.*,}']</code> A list of glob of filepath to include. It is automatically generated from <code>suffixes</code> and <code>extensions</code>."},{"location":"features/jinja/#jinja-template-processing","title":"Jinja template processing","text":"<p>When running the <code>ddb configuration</code> command, ddb will look for files matching the list of <code>includes</code> configuration and  not in the <code>excludes</code> list.</p> <p>For each file, it will be processed into his final form. </p> <p>In those templates, you can retrieve ddb configuration values simply using the full name of the variable.</p> <p>But ... how do I retrieve the full name of a variable ?</p> <p>Well, as you might already know, you can execute <code>ddb config</code> in order to check the configuration used in ddb.</p> <p>But if you append <code>--variables</code> to this command, you will have the variable name to include in your template !</p> <p>dotenv <code>.env</code> configuration file generation</p> <p>You want to generate a <code>.env</code> file based on your current ddb configuration.</p> <p>You can create a <code>.env.jinja</code> and replace the parts you need to fill with those variables:</p> <pre><code>APP_ENV=dev\nAPP_SECRET=4271e37e11180de028f11a132b453fb6\nCORS_ALLOW_ORIGIN=^https?://{{core.domain.sub}}\\.{{core.domain.ext}}$\nDATABASE_URL=mysql://ddb:ddb@db:3306/ddb\nMAILER_URL=smtp://mail\n</code></pre> <p>As you can see, we have configured <code>CORS_ALLOW_ORIGIN</code> using ddb configuration variables.</p> <p>Now, we can run <code>ddb configure</code>. After the file have been processed, <code>.env</code> file is generated next to <code>.env.jinja</code>  template file.</p> <p>With <code>core.domain.sub</code> set to <code>ddb</code> and <code>core.domain.ext</code> set to 'test', you will get the following content :</p> <pre><code>APP_ENV=dev\nAPP_SECRET=4271e37e11180de028f11a132b453fb6\nCORS_ALLOW_ORIGIN=^https?://ddb\\.test$\nDATABASE_URL=mysql://ddb:ddb@db:3306/ddb\nMAILER_URL=smtp://mail\n</code></pre> <p>This way, you don't need to manually update <code>.env</code> file manually and can keep the whole project configuration  centralized.</p>"},{"location":"features/jsonnet/","title":"Jsonnet","text":"<p>Jsonnet is embedded into ddb. You only have to use the right file extension for ddb to  process it through the appropriate template engine.</p> <p>Mainly used to generate <code>docker-compose.yml</code> configuration, you can still use it for any other json or yaml based  templating purpose.</p> <p>When a jsonnet template is processed, ddb use it's configuration as context, so you can use them as you need using  jsonnet standard library function.</p> <pre><code>std.extVar(\"&lt;name of the configuration variable&gt;\")\n</code></pre> <p>Run <code>ddb configure</code> to evaluate templates and generate target files.</p> <p>Feature configuration (prefixed with <code>jsonnet.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>suffixes</code> string[]<code>['.jsonnet']</code> A list of filename suffix to include. <code>extensions</code> string[]<code>['.*', '']</code> A list of glob of supported extension. <code>excludes</code> string[]<code>[]</code> A list of glob of filepath to exclude. <code>docker</code> Docker Docker related configuration. Advanced Property Type Description <code>includes</code> string[]<code>['*.jsonnet{.*,}']</code> A list of glob of filepath to include. It is automatically generated from <code>suffixes</code> and <code>extensions</code>. <p>Docker configuration (prefixed with <code>jsonnet.docker.</code>)</p> Simple Property Type Description <code>compose</code> Compose docker-compose defaults. <code>networks</code> Networks <code>ddb.Networks()</code> defaults. <code>build</code> Build <code>ddb.Build()</code> defaults. <code>service</code> Service <code>ddb.Service()</code> defaults. <code>expose</code> Expose <code>ddb.Expose()</code> defaults. <code>registry</code> Registry Image registry settings. <code>user</code> User <code>ddb.User()</code> defaults. <code>binary</code> Binary <code>ddb.Binary()</code> defaults. <code>virtualhost</code> VirtualHost <code>ddb.VirtualHost()</code> defaults. <code>xdebug</code> XDebug <code>ddb.XDebug()</code> defaults. <p>Docker Compose configuration (prefixed with <code>jsonnet.docker.compose.</code>)</p> Simple Property Type Description <code>project_name</code> string Docker compose project name <code>network_name</code> string Docker compose network name <code>version</code> string<code>3.7</code> YAML File version to use <code>excluded_services</code> string[]<code>[]</code> Services to exclude (Block list). <code>included_services</code> string[]<code>[]</code> Services to include (Allow list). <p>Docker Networks configuration (prefixed with <code>jsonnet.docker.networks</code>)</p> Simple Property Type Description <code>names</code> Dict[str, str] Additional external networks <p>Docker Build configuration (prefixed with <code>jsonnet.docker.build.</code>)</p> Simple Property Type Description <code>cache_from_image</code> boolean<code>False</code> Add cache_from_image to configuration. <code>context.base_directory</code> string<code>.docker</code> Base directory for build context. <code>context.use_project_home</code> boolean<code>False</code> Use project home directory as build context. <code>image_tag_from</code> boolean string<code>False</code> Advanced Property Type Description <code>image_tag</code> string Image tag value. <p>Docker Service configuration (prefixed with <code>jsonnet.docker.service.</code>)</p> Simple Property Type Description <code>restart</code> string<code>unless-stopped</code><code>no</code> The restart policy to use for all services. Can be <code>no</code>, <code>always</code>, <code>on-failure</code> or <code>unless-stopped</code>. Default value is <code>unless-stopped</code>, unless <code>core.env.current</code> is set to <code>dev</code> then it's set to <code>no</code>. Advanced Property Type Description <code>init</code> string <p>Docker Expose configuration (prefixed with <code>jsonnet.docker.expose.</code>)</p> Property Type Description <code>disabled</code> boolean<code>False</code> Should <code>ddb.Expose()</code> perform nothing ? <code>port_prefix</code> integer<code>&lt;based on core.project.name&gt;</code> Port prefix. <p>Docker Mount configuration (prefixed with <code>jsonnet.docker.mount.</code>)</p> Property Type Description <code>disabled</code> boolean<code>False</code> Should <code>ddb.Expose()</code> perform nothing ? <code>directory</code> string Base directory for all named volume mounts, absolute or relative to project home. <code>directories</code> dict[string, string] Directories for named volume mounts, absolute or relative to project home. key is the volume name, value is the local path to mount. <p>Docker Registry configuration (prefixed with <code>jsonnet.docker.registry.</code>)</p> Property Type Description <code>name</code> string Registry name. <code>repository</code> string Registry repository. <p>Docker User configuration (prefixed with <code>jsonnet.docker.user.</code>)</p> Simple Property Type Description <code>uid</code> string The user UID to use inside a container. <code>gid</code> string The user GID to use inside a container. <code>name</code> string The host username that will get converted to UID. <code>group</code> string The host groupname that will get converted to GID. Internal <p>| <code>name_to_uid</code> | Dict[str, integer] | Mapping of user names to uid. | | <code>group_to_gid</code> | Dict[str, integer] | Mapping of group names to gid. |</p> <p>Docker Binary configuration (prefixed with <code>jsonnet.docker.binary.</code>)</p> Property Type Description <code>disabled</code> boolean<code>False</code> Should binary generation be disabled ? <code>global</code> boolean Should binaries be generated as global by default ? <p>Docker VirtualHost configuration (prefixed with <code>jsonnet.docker.virtualhost.</code>)</p> Simple Property Type Description <code>https</code> string Should services be available as HTTPS ? If unset, it is available as both HTTP and HTTPS. <code>redirect_to_https</code> string Should services redirect to HTTPS when requested on HTTP ? <code>redirect_to_path_prefix</code> string Should services redirect to path_prefix when requested on root path ? <code>certresolver</code> string certresolver to use. <code>letsencrypt</code> is supported when using traefik reverse proxy. Internal <p>| <code>type</code> | string<code>traefik</code> (when available) | Type of reverse proxy to use. | | <code>network_id</code> | string<code>reverse-proxy</code> | Network id used. |</p> <p>Docker XDebug configuration (prefixed with <code>jsonnet.docker.xdebug.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should debug features be generated in <code>docker-compose.yml</code> by jsonnet feature ? <code>host</code> string<code>${jsonnet}</code> The host to connect back for debug features. <code>version</code> string XDebug version to configure (<code>2</code> or <code>3</code>). If unset, both XDebug 2 and 3 configurations will generated in a merged object. <code>session</code> string<code>${core.project.name}</code> XDebug session (v3) and/or idekey/serverName (v2) to configure. You can set this value explicitly to set env vars <code>XDEBUG_SESSION</code> (v3) and/or <code>PHP_IDE_CONFIG</code>/<code>XDEBUG_CONFIG</code> (v2). <code>mode</code> string<code>debug</code> XDebug mode (v3 only)."},{"location":"features/jsonnet/#djp-packages-ddb-jsonnet-packages","title":"djp packages (ddb jsonnet packages)","text":"<p>You can include one or many ddb jsonnet packages, aka djp, into <code>ddb.Compose</code> configuration using <code>ddb.with()</code>.</p> <p>More information: Djp packages</p>"},{"location":"features/jsonnet/#docker-compose-jsonnet-library","title":"Docker-compose jsonnet library","text":"<p>Jsonnet feature provides a library with handful functions to help generate <code>docker-compose.yml</code>.</p> <p>In order to make those available, you need to import the library:</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n</code></pre>"},{"location":"features/jsonnet/#ddbcompose","title":"ddb.Compose()","text":"<p>This function defines the main entrypoint to generate a docker-compose configuration. </p> <p>Parameters</p> Property Type Description <code>config</code> object Docker compose configuration <code>networks_names</code> dict[str, str]<code>${jsonnet.docker.networks.names}</code> Network id to name mapping <p>Example</p> <pre><code>ddb.Compose()\n</code></pre> <p>without any service configuration will produce</p> <pre><code>networks:\n  reverse-proxy:\n    external: true\n    name: reverse-proxy\n</code></pre> <p>Example</p> <pre><code>ddb.Compose({\n    services: {\n        db: {\n            image: \"postgres\"\n        }\n    }\n})\n</code></pre> <p>with a configuration with produce</p> <pre><code>networks:\n  reverse-proxy:\n    external: true\n    name: reverse-proxy\nservices:\n  db:\n    image: \"postgres\"\n</code></pre>"},{"location":"features/jsonnet/#ddbbuild","title":"ddb.Build()","text":"<p>This function generates a service configuration from a <code>Dockerfile</code> available in <code>.docker/&lt;service&gt;</code> directory.</p> <p>If a docker registry is configured inside docker feature, <code>image</code> configuration will also be generated from the  service name.</p> <p>Parameters</p> Property Type Description <code>name</code> string Generate a build configuration based on given name. <code>image</code> string<code>&lt;name&gt;</code> The name of the image. If <code>registry_name</code> and/or <code>registry_name</code> settings are defined, it will generate the full image name. <code>cache_from_image</code> boolean<code>${jsonnet.docker.build.cache_from_image}</code> If set to true and docker registry is defined, it will generate the <code>build.cache_from</code> configuration uri. <code>context_base_directory</code> boolean<code>${jsonnet.docker.build.context.base_directory}</code> Build context base directory. <code>context_use_project_home</code> boolean<code>${jsonnet.docker.build.context.use_project_home}</code> Use project home directory as context. <code>restart</code> string<code>${jsonnet.docker.service.restart}</code> Service restart policy. <code>init</code> boolean<code>${jsonnet.docker.service.init}</code> Service init. <code>registry_name</code> string<code>${jsonnet.docker.registry.name}</code> Name of the docker image registry. <code>registry_repository</code> string<code>${jsonnet.docker.registry.repository}</code> Repository in the docker image registry. <code>image_tag</code> boolean string<code>${jsonnet.docker.build.image_tag}</code> <p>Example with a registry defined</p> <p>if ddb.yml contains the following</p> <pre><code>jsonnet:\n  docker:\n    registry:\n      name: docker.io\n      repository: project\n    build:\n      image_tag_from: True\n</code></pre> <pre><code>ddb.Build(\"db\")\n</code></pre> <p>will produce</p> <pre><code>build:\n  context: .docker/db\n  cache_from: docker.io/project/db:master\nimage: docker.io/project/db:master\n</code></pre>"},{"location":"features/jsonnet/#ddbimage","title":"ddb.Image()","text":"<p>This function generates a service configuration based on an external image.</p> <p>It will also add the <code>init</code> to true and <code>restart</code> configurations for the service.</p> <p>The <code>restart</code> configuration will be set with the <code>jsonnet.docker.service.restart</code> ddb configuration.</p> <p>Parameters</p> Property Type Description <code>image</code> string* The name of the image to use. <code>restart</code> string<code>${jsonnet.docker.service.restart}</code> Service restart policy. <code>init</code> boolean<code>${jsonnet.docker.service.init}</code> Service init. <p>Example</p> <pre><code>ddb.Image(\"nginx:latest\")\n</code></pre> <p>will produce</p> <pre><code>image: nginx:latest\nrestart: no\ninit: true\n</code></pre>"},{"location":"features/jsonnet/#ddbexpose","title":"ddb.Expose()","text":"<p>This function generates an exposed port inside a service.</p> <p>It use <code>jsonnet.docker.expose.port_prefix</code> to generate a fixed mapped port in order to avoid port collisions between projects.</p> <p>Parameters</p> Property Type Description <code>container_port</code> string|integer* Container port number to expose. <code>host_port_suffix</code> string End of the mapped port on host. from <code>1 to 99</code>. If <code>null</code>, use last 2 digits of <code>container_port</code> value. <code>protocol</code> string The protocol to use. Can be <code>null</code>, <code>tcp</code> or <code>udp</code>. <code>port_prefix</code> string|integer<code>${jsonnet.docker.expose.port_prefix}</code> Port prefix. <p>Example</p> <pre><code>ddb.Expose(21) +\nddb.Expose(22, null, \"udp\") +\nddb.Expose(23, 99, \"tcp\")\n</code></pre> <p>will produce</p> <pre><code>ports:\n  - '14721:21'\n  - '14722:22/udp'\n  - '14799:23/tcp'\n</code></pre> <p><code>147</code> is <code>jsonnet.docker.expose.port_prefix</code> configuration value.</p> <p>You can expose the same container port many times</p> <p>To resolve published port collisions, when a published port is already consumed by another ddb.Expose() invocation,  ddb automatically increments the published port until it's available.</p> <pre><code>ddb.Expose(21) +\nddb.Expose(21) +\n</code></pre> <p>will produce</p> <pre><code>ports:\n  - '14721:21'\n  - '14722:21'\n</code></pre> <p><code>147</code> is <code>jsonnet.docker.expose.port_prefix</code> configuration value.</p>"},{"location":"features/jsonnet/#ddbuser","title":"ddb.User()","text":"<p>This function generates the <code>user</code> configuration for a Service.</p> <p>In ddb, it is mainly use for fixuid automatic integration</p> <p>Parameters</p> Property Type Description <code>uid</code> string<code>${jsonnet.docker.user.uid}</code><code>${~jsonnet.docker.user.name}</code> UID of user running the container. <code>gid</code> string<code>${jsonnet.docker.user.gid}</code><code>${~jsonnet.docker.user.group}</code> GID of user running the container. <p>Example</p> <pre><code>ddb.User()\n</code></pre> <p>will produce</p> <pre><code>user: 1000:1000\n</code></pre> <p><code>userNameToUid</code> and <code>groupNameToGid</code> to retrieve host uid/gid from names</p> <pre><code>ddb.User(gid=ddb.groupNameToGid(\"docker\"))\n</code></pre> <p>will produce</p> <pre><code>user: 1000:998\n</code></pre> <p>when <code>getent group docker</code> returns <code>docker:x:998:</code> on the host.</p> <p>It can be used when you need the container to access the docker socket through a volume mount.</p>"},{"location":"features/jsonnet/#ddbvirtualhost","title":"ddb.VirtualHost()","text":"<p>This function generates service configuration used for reverse-proxy auto-configuration.</p> <p>The output generated depends on the <code>jsonnet.docker.virtualhost.type</code> ddb configuration. Currently, only traefik is supported. If this configuration is anything else, there will be no output.</p> <p>Parameters</p> Property Type Description <code>port</code> string* HTTP port inside the container. <code>hostname</code> string* Hostname that will be exposed. <code>name</code> string Unique name for this VirtualHost. <code>network_id</code> string<code>${jsonnet.docker.virtualhost.network_id}</code> The reverse-proxy network id. <code>certresolver</code> string<code>${jsonnet.docker.virtualhost.certresolver}</code> certresolver to use inside reverse proxy (traefik). <code>letsencrypt</code> is supported when using <code>traefik</code> feature. <code>router_rule</code> string <code>https</code> string<code>${jsonnet.docker.virtualhost.https}</code> Should service be available as HTTPS ? If unset, it is available as both HTTP and HTTPS. <code>redirect_to_https</code> string<code>${jsonnet.docker.virtualhost.redirect_to_https}</code> Should service redirect to HTTPS when requested on HTTP ? <code>path_prefix</code> string Path prefix of this virtualhost <code>redirect_to_path_prefix</code> boolean Redirect to configured path prefix <p>Example with traefik as reverse proxy</p> <pre><code>ddb.VirtualHost(\"80\", \"your-project.test\", \"app\")\n</code></pre> <p>will produce</p> <pre><code>labels:\n  traefik.enable: 'true'\n  traefik.http.routers.your-project-app-tls.rule: Host(`your-project.test`)\n  traefik.http.routers.your-project-app-tls.service: your-project-app\n  traefik.http.routers.your-project-app-tls.tls: 'true'\n  traefik.http.routers.your-project-app.rule: Host(`your-project.test`)\n  traefik.http.routers.your-project-app.service: your-project-app\n  traefik.http.services.your-project-app.loadbalancer.server.port: '80'\nnetworks:\n- default\n- reverse-proxy\n</code></pre>"},{"location":"features/jsonnet/#ddbbinary","title":"ddb.Binary()","text":"<p>Binary allow the creation of alias for command execution inside the service.</p> <p>Parameters</p> Property Type Description <code>name</code> string* Binary name. This will be the command you type in shell (see shell feature). <code>workdir</code> string Default container directory to run the command. In most case, it should match the service workdir. <code>args</code> string<code>&lt;name&gt;</code> Command to execute inside the container. <code>options</code> string Options to add to the command. <code>options_condition</code> string Add a condition to be evaluated to make <code>options</code> optional. If condition is defined and evaluated, <code>options</code> are not added to the command. <code>exe</code> boolean<code>false</code> Launch command with docker-compose <code>exec</code> instead of <code>run</code>. <code>entrypoint</code> string Override entrypoint for this binary. <code>global</code> boolean<code>false</code> Creates a global binary shim in <code>~/.docker-devbox/.bin</code> directory so it can be runned everywhere. <code>condition</code> string Add a condition for the command to be enabled. If condition is defined and evaluated to false, command won't be used by run feature. <p>Example</p> <pre><code>ddb.Binary(\"npm\", \"/app\", \"npm\", \"--label traefik.enable=false\", '\"serve\" not in args')\n</code></pre> <p>will produce</p> <pre><code>labels:\n    ddb.emit.docker:binary[npm](args): npm\n    ddb.emit.docker:binary[npm](name): npm\n    ddb.emit.docker:binary[npm](options): --label traefik.enable=false\n    ddb.emit.docker:binary[npm](options_condition): '\"serve\" not in args'\n    ddb.emit.docker:binary[npm](workdir): /app\n</code></pre> <p>Register the same command many times</p> <p>You may want is some projects to have many binaries defined for the same command, i.e many version of <code>composer</code> or  <code>npm</code>.</p> <p>It is possible to implement a condition on each Binary in order to enable one binary in a project directory, and  another one in another project directory.</p> <p>Each binary sharing the same name should be defined in distinct services though, as it doesn't make sense to define  the same command on the same service.</p> <p>You can find more information in run feature: Register many binaries for the same command.</p>"},{"location":"features/jsonnet/#ddbxdebug-php","title":"ddb.XDebug() (PHP)","text":"<p>This function generates <code>environment</code> configuration used for XDebug (PHP Debugger).</p> <p>If <code>jsonnet.docker.xdebug.disabled</code> is set to <code>true</code>, the function returns an empty object.</p> <p>It will use the following <code>ddb</code> configuration to generate appropriate <code>environment</code>:</p> <ul> <li><code>core.project.name</code>: <ul> <li>XDebug 2: set <code>serverName</code> and <code>idekey</code></li> <li>XDebug 3: set <code>XDEBUG_SESSION</code></li> </ul> </li> <li><code>jsonnet.docker.xdebug.host</code>: <ul> <li>XDebug 2: set <code>remote_host</code></li> <li>XDebug 3: set <code>client_host</code></li> </ul> </li> </ul> <p>Parameters</p> Property Type Description <code>version</code> string<code>${jsonnet.docker.xdebug.version}</code> XDebug version to configure (<code>2</code> or <code>3</code>). If unset, both XDebug 2 and 3 configurations will generated in a merged object. <code>session</code> string<code>${jsonnet.docker.xdebug.session}</code> XDebug session (v3) and/or idekey/serverName (v2) to configure. You can set this value explicitly to set env vars <code>XDEBUG_SESSION</code> (v3) and/or <code>PHP_IDE_CONFIG</code>/<code>XDEBUG_CONFIG</code> (v2). <code>mode</code> string<code>${jsonnet.docker.xdebug.mode}</code> XDebug mode (v3 only). <p>Example</p> <pre><code>ddb.XDebug()\n</code></pre> <p>will produce</p> <pre><code>environment:\n  PHP_IDE_CONFIG: serverName=project-name\n  XDEBUG_CONFIG: remote_enable=on remote_autostart=off idekey=project-name remote_host=192.168.85.1\n</code></pre>"},{"location":"features/jsonnet/#ddbenvis","title":"ddb.env.is()","text":"<p>This function allows you to check if the given environment is the current one, i.e. is equals to <code>core.env.current</code>.</p> <p>It does not have any input parameter and returns boolean.</p> <p>This can be used to add environment condition to a service activation, a specific configuration,... </p> <p>Parameters</p> Property Type Description <code>env</code> string Environment name to verify. <p>Example</p> <p>With the following configuration : <pre><code>core:\n  env:\n    current: dev\n</code></pre></p> <ul> <li><code>ddb.env.is(\"prod\")</code> =&gt; <code>false</code></li> <li><code>ddb.env.is(\"dev\")</code> =&gt; <code>true</code></li> </ul>"},{"location":"features/jsonnet/#ddbenvcurrent-ddbenvavailable","title":"ddb.env.current / ddb.env.available","text":"<p>Shortcuts for <code>std.extVar(\"core.env.current\")</code> and <code>std.extVar(\"core.env.available\")</code>.</p>"},{"location":"features/jsonnet/#ddbprojectname-ddbdomain","title":"ddb.projectName / ddb.domain","text":"<p>Shortcuts for <code>std.extVar(\"core.project.name\")</code>, <code>std.extVar(\"core.domain.value\")</code></p>"},{"location":"features/jsonnet/#ddbsubdomain","title":"ddb.subDomain()","text":"<p>Builds a subdomain string from default domain name of another domain name.</p> <p>Example</p> <p>With the following configuration : <pre><code>core:\n  domain:\n    sub: domain\n    ext: tld\n</code></pre></p> <ul> <li><code>ddb.subDomain(\"mailcatcher\")</code> =&gt; <code>mailcatcher.domain.tld</code></li> <li><code>ddb.subDomain(\"mailcatcher\", \"inetum.world\")</code> =&gt; <code>mailcatcher.inetum.world</code></li> </ul>"},{"location":"features/jsonnet/#advanced-functions","title":"Advanced functions","text":"<p>Those functions are for advanced configuration and should not be used in most common cases.</p> Advanced"},{"location":"features/jsonnet/#ddbservicename","title":"ddb.ServiceName()","text":"<p>This function generates the right service name for a service.</p> <p>The main purpose is to have more easy way to manage Labels for traefik and easily add a middleware to a specific  service.</p> <p>It concatenates the given name with <code>${core.project.name}</code>.</p> <p>Parameters</p> Property Type Description <code>name</code> string* Name of the service. <p>Example</p> <p>With a project named \"ddb\"  <pre><code>ddb.ServiceName(\"test\")\n</code></pre> will return ```yaml ddb-test</p>"},{"location":"features/jsonnet/#ddbbinarylabels","title":"ddb.BinaryLabels()","text":"<p>BinaryLabels is mostly the same as Binary but output configuration without the <code>labels:</code> part, so you can add  it directly to your own labels block.</p> <p>Parameters</p> Property Type Description <code>name</code> string* Binary name. This will be the command you type in shell (see shell feature). <code>workdir</code> string Default container directory to run the command. In most case, it should match the service workdir. <code>args</code> string<code>&lt;name&gt;</code> Command to execute inside the container. <code>options</code> string Options to add to the command. <code>options_condition</code> string Add a condition to be evaluated to make <code>options</code> optional. If condition is defined and evaluated, <code>options</code> are not added to the command. <p>Example</p> <pre><code>ddb.BinaryLabels(\"npm\", \"/app\", \"npm\", \"--label traefik.enable=false\", '\"serve\" not in args')\n</code></pre> <p>will produce</p> <pre><code>ddb.emit.docker:binary[npm](args): npm\nddb.emit.docker:binary[npm](name): npm\nddb.emit.docker:binary[npm](options): --label traefik.enable=false\nddb.emit.docker:binary[npm](options_condition): '\"serve\" not in args'\nddb.emit.docker:binary[npm](workdir): /app\n</code></pre>"},{"location":"features/jsonnet/#ddbbinaryoptions","title":"ddb.BinaryOptions()","text":"<p>BinaryOptions allow you to add options to service's binary previously declared.</p> <p>Parameters</p> Property Type Description <code>name</code> string* Binary name. This will be the command you type in shell (see shell feature). <code>options</code> string Options to add to the command. <code>options_condition</code> string Add a condition to be evaluated to make <code>options</code> optional. If condition is defined and evaluated, <code>options</code> are not added to the command. <p>Example</p> <pre><code>ddb.BinaryOptions(\"npm\", \"--label traefik.enable=false\", '\"serve\" not in args')\n</code></pre> <p>will produce</p> <pre><code>labels:\n    ddb.emit.docker:binary[npm](options): --label traefik.enable=false\n    ddb.emit.docker:binary[npm](options_condition): '\"serve\" not in args'\n</code></pre>"},{"location":"features/jsonnet/#ddbbinaryoptionslabels","title":"ddb.BinaryOptionsLabels()","text":"<p>BinaryOptionsLabels is mostly the same as BinaryOptions but output configuration without the  <code>labels:</code> part, so you can add it directly to your own labels block.</p> <p>Parameters</p> Property Type Description <code>name</code> string* Binary name. This will be the command you type in shell (see shell feature). <code>options</code> string Options to add to the command. <code>options_condition</code> string Add a condition to be evaluated to make <code>options</code> optional. If condition is defined and evaluated, <code>options</code> are not added to the command. <p>Example</p> <pre><code>ddb.BinaryOptions(\"npm\", \"--label traefik.enable=false\", '\"serve\" not in args')\n</code></pre> <p>will produce</p> <pre><code>labels:\n    ddb.emit.docker:binary[npm](options): --label traefik.enable=false\n    ddb.emit.docker:binary[npm](options_condition): '\"serve\" not in args'\n</code></pre>"},{"location":"features/jsonnet/#ddbpath","title":"ddb.path","text":"<p>Path is easy access to ddb configuration paths <code>core.path</code> values.</p> <p>It is mostly used to add a folder as volume to service.</p>"},{"location":"features/jsonnet/#ddbenvindex","title":"ddb.env.index()","text":"<p>This function allows you to get the index of the given environment in the list <code>core.env.available</code>.</p> <p>If there is no parameter given, it provides the index of the current one. </p> <p>This can be used to add environment condition to a service activation, a specific configuration,... </p> <p>Parameters</p> Property Type Description <code>env</code> string Environment name to verify. <p>Example</p> <p>With the following configuration : <pre><code>core:\n  env:\n    available:\n    - prod\n    - stage\n    - ci\n    - dev\n    current: dev\n</code></pre></p> <ul> <li><code>ddb.env.index()</code> will return 3</li> <li><code>ddb.env.index(\"ci\")</code> will return 2</li> </ul>"},{"location":"features/jsonnet/#ddbmergeall","title":"ddb.mergeAll()","text":"<p>This function merge an array of objects.</p> <p>Parameters</p> Property Type Description <code>object_array</code> dict[] the array of objects to merge. <p>Example</p> <p>With the following jsonnet declaration : <pre><code>local sites = ['www', 'api'];\n\n{\n    \"web\": ddb.Build(\"web\")\n        + ddb.mergeAll([ddb.VirtualHost(\"80\", std.join('.', [site, \"domain.tld\"]), site) for site in sites])\n        + {\n            \"volumes\": [\n                ddb.path.project + \"/.docker/web/nginx.conf:/etc/nginx/conf.d/default.conf:rw\",\n                ddb.path.project + \":/var/www/html:rw\"\n            ]\n        }\n}\n</code></pre></p> <p>The result will be <pre><code>labels: \n  traefik.enable: \"true\"\n  traefik.http.routers.your-project-api-tls.rule: Host(`api.domain.tld`)\n  traefik.http.routers.your-project-api-tls.service: your-project-api\n  traefik.http.routers.your-project-api-tls.tls: \"true\"\n  traefik.http.routers.your-project-api.rule: Host(`api.domain.tld`)\n  traefik.http.routers.your-project-api.service: your-project-api\n  traefik.http.services.your-project-api.loadbalancer.server.port: '80'\n  traefik.http.routers.your-project-www-tls.rule: Host(`www.domain.tld`)\n  traefik.http.routers.your-project-www-tls.service: your-project-www\n  traefik.http.routers.your-project-www-tls.tls: \"true\"\n  traefik.http.routers.your-project-www.rule: Host(`www.domain.tld`)\n  traefik.http.routers.your-project-www.service: your-project-www\n  traefik.http.services.your-project-www.loadbalancer.server.port: '80'\n</code></pre></p>"},{"location":"features/jsonnet/#ddbpathmappath","title":"ddb.path.mapPath()","text":"<p>Get the mapped value of a given filepath according to mappings configured in <code>docker.path_mapping</code>.</p> <p>Parameters</p> Property Type Description <code>path</code> string Source path."},{"location":"features/jsonnet/#ddbfile","title":"ddb.File","text":"<p>Wrap a string matching a Filesystem path into a File object containing <code>name</code> and <code>parent</code>. <code>name</code> match the filename of  this path, and <code>parent</code> is another File object matching the parent path.</p> <p>Example</p> <ul> <li><code>ddb.File(\".docker/postgres/djp.libsonnet\")</code> =&gt; <code>{name: 'djp.libjsonnet', parent: {name: postgres, parent: {name: .docker}}}</code></li> </ul>"},{"location":"features/permissions/","title":"Permissions","text":"<p>Permissions can be a bit tedious to update. Plus, you might want to define the right permissions on a file once and for all.</p> <p>Well, this feature is made for you!</p> <p>Feature configuration (prefixed with <code>permissions.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>specs</code> dict[string, string] Key of the dict is a filepath glob matching the files to change, value is a chmod-like permission modifier like <code>+x</code> or <code>400</code>. <p>Add executable permission to each file in <code>bin/</code> directory</p> <pre><code>permissions:\n  specs:\n    \"bin/*\": \"+x\"\n</code></pre>"},{"location":"features/permissions/#permission-update","title":"Permission update","text":"<p>The permission management is performed for <code>file:found</code> and <code>file:generated</code> events.</p> <p>When one of those is raised, the system check if the file associated to the event is part of the <code>specs</code> configuration. If it is part of it, the feature will update the permissions of the file accordingly to the chmod modifier defined.</p>"},{"location":"features/run/","title":"Run","text":"<p>As ddb is a way to simplify the way to work with docker, this feature is a very useful one. </p> <p>If you are working with the jsonnet feature and the Binary function in your  <code>docker-compose.yml.jsonnet</code>, <code>run</code> feature allow you to run commands in your docker container just as if it  was a native system command.</p> <p>Feature configuration (prefixed with <code>run.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ?"},{"location":"features/run/#run-command-in-container-as-if-it-was-native","title":"Run command in container as if it was native","text":"<p>As said in introduction, the purpose of the <code>run</code> feature is to allow you to run commands in your docker container  just as if it was a native system command.</p> <p>In order to do so, the feature will look in the project configuration for the binary you are trying to run. Then, it  will generate the right docker-compose command to be executed.</p> <p>But, as we are lazy, it was not enough.  So, when you create a project  the docker will trigger the shell one, which will generate a  binary file for the current project. This file will simply execute the <code>ddb run &lt;binary_name&gt;</code> with the input argument  and automatically execute the result.</p> <p>The combination of the previously named feature with the current one will grant you the capability to execute <code>npm</code> just as you would do with a system installation of it.</p> <p>How to run <code>npm</code> or <code>composer</code> command ?</p> <p>If you follow the Guide for PostgreSQL, Symfony, VueJS, you will be able to execute <code>ddb run npm</code> which will output the following command <code>docker-compose run --rm --workdir=/app/. node npm</code>.</p> <p>But, as explained, ddb will also create a binary shim for npm so you can run it like a native command, which is  far more easy to use for everyone. Run $(ddb activate), and then <code>npm</code> is available right in your shell path.</p>"},{"location":"features/run/#register-many-binaries-for-the-same-command","title":"Register many binaries for the same command","text":"<p>When a binary shim is invoked, it runs <code>ddb run &lt;command-name&gt;</code> under the hood.</p> <p>All declared binaries matching the command name with a <code>condition</code> defined will have their <code>condition</code> evaluated, and  the first one returning <code>True</code> is used to generate the effective command output. </p> <p>If no <code>condition</code> match, the first remaining one is used.</p> <p>If no binary is finally found, the command is wrapped into equivalent of <code>$(ddb deactivate)</code>/<code>$(ddb activate)</code> instruction so it runs on the host system.</p>"},{"location":"features/shell/","title":"Shell","text":"<p>The shell feature manages OS/Shell specific behaviors (Windows, Linux/Unix).</p> <p>For instance, generated binary shims are bash executables on Linux, but .bat files on Windows.</p> <p>Feature configuration (prefixed with <code>symlinks.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>aliases</code> dict[string, string] Allow the creation of aliases. <code>global_aliases</code> string[] Aliases matching those names are available globally instead of inside project only. Advanced Property Type Description <code>envignore</code> string[]<code>[\"PYENV_*\", \"_\", \"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PWD\"]</code> When activating ddb for a project via <code>$(ddb activate)</code>, environment variables are saved before being updated. This list is those who will not be saved and updated by the command. <code>path.directories</code> string[]<code>[\".bin\", \"bin\"]</code> List of directories to add to <code>PATH</code> environment variable when running <code>$(ddb activate)</code>. The first one from this list is also used as root folder for binaries and aliases shims generation. <code>path.prepend</code> bollean<code>true</code> Should paths declared in <code>path.directories</code> be placed at the begging of <code>PATH</code> environment variable. If set to <code>false</code>, it will be added to the end. Internal Property Type Description <code>shell</code> string<code>bash</code> (linux)<code>bash</code> (macos)<code>cmd</code> (windows) Type of shell to work with. Currently, only <code>bash</code> and <code>cmd</code> (windows) are supported."},{"location":"features/shell/#environment-activation","title":"Environment activation","text":"<p>ddb is generating configurations for your project and brings docker container binaries right in your development  environment. </p> <p>If you have registered binary inside <code>docker-compose.yml.jsonnet</code>, you can found binary shims inside <code>.bin</code> directory. It means that you need to be in the root folder of your project, or to add the full path to the executable file if you  are not.</p> <p>The best solution is to update your <code>PATH</code>. ddb provides you a way to do it by running the <code>$(ddb activate)</code> command in your shell.</p> <p>This command will generate environment variables updates commands and execute them, including <code>PATH</code> update for easy  to access stored in the <code>.bin</code> folder of your project from anywhere.</p> <p>Changes to environment are local and not persistent</p> <p>The modification of the shell environment is local to your current shell session.  It means that if you open a new one, the environment will be the same as before run the <code>$(ddb activate)</code> command.</p> <p>But what happen when switching to another project ?</p> <p>The environment variables will still be configured for the project you ran the command for. So before leaving his directory, you will need to run <code>$(ddb deactivate)</code> command.</p> <p>It will unload the project specific environment variable configuration and restore it to the initial state.</p> <p>Now you can move to the new project directory and run <code>$(ddb activate)</code> once again, and so on.</p> <p>If you are as lazy as we are ...</p> <p>In order to automate this process, check SmartCD Feature. It will run those commands when entering  and leaving the project directory containing the <code>ddb.yml</code> file.</p>"},{"location":"features/shell/#docker-binary-shims","title":"Docker binary shims","text":"<p>If you are using Jsonnet templates with <code>ddb.Binary()</code> function, this means that you want to have  simple access to command execution inside docker containers. </p> <p>As the generation of the shims depends on your specific shell (cmd.exe, bash, ...), it is handled by the shell feature.</p> <p>Each declared binary inside <code>.jsonnet</code> file generates a shim inside the <code>.bin</code> project directory  (directory configured in <code>shell.path.directories[0]</code>), and available in your shell after running <code>$(ddb activate)</code>.</p> <p>Access to PostgreSQL commands the native way</p> <p>If you have a PostgreSQL container, you may need to run commands like ...   - <code>psql</code> - PostgreSQL client binary.   - <code>pg_dump</code> - PostgreSQL command line tool to export data to a file.</p> <p>Instead of writing a long and impossible to remember docker commmand, you should declare them using <code>ddb.Binary()</code>  function in <code>docker-compose.yml.jsonnet</code> file.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n    services: {\n        db: ddb.Image(\"postgres\") +\n            ddb.Binary(\"psql\", \"/project\", \"psql --dbname=postgresql://postgres:ddb@db/postgres\") +\n            ddb.Binary(\"pg_dump\", \"/project\", \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\") +\n          {\n            environment+: {POSTGRES_PASSWORD: \"ddb\"},\n            volumes+: [\n              'db-data:/var/lib/postgresql/data',\n              ddb.path.project + ':/project'\n            ]\n          }\n    }\n})\n</code></pre> <p>Then, run <code>ddb configure</code>, which will generate executable shim file in the <code>.bin</code> folder.</p> <p>Finaly, run <code>$(ddb activate)</code> to update your <code>PATH</code> and bring those commands to your local environment.</p> <p>Now, <code>psql</code> and <code>pg_dump</code> are available as if they were native commands.</p>"},{"location":"features/shell/#aliases-management","title":"Aliases Management","text":"<p>On your environment, aliases can be manually created in order to save time on repetitive commands execution or on long instructions. Some of those aliases are really useful only in a specific project context.</p> <p>The shell feature allows you to create your own aliases which will be generated the same way as  jsonnet ddb.Binary().</p> <p>In order to declare them, you need to update the ddb configuration with the list of aliases : </p> <pre><code>shell:\n    aliases:\n      myAlias: theLongCommandToExecute\n</code></pre> <p>Make a composer dependency available globally</p> <p>When you are working on PHP/Composer project, some commands are available inside <code>vendor</code> path.</p> <p>For drupal developers, Drush commands is available through <code>vendor/drush/drush/drush</code>  binary.</p> <p>Instead of writing the full path of this binary each time, you can declare an alias in ddb configuration. </p> <pre><code>shell:\n  aliases:\n    drush: vendor/drush/drush/drush\n</code></pre> <p>A binary shim will be created and added to the <code>PATH</code> thanks to ddb so you will able to use it from you project  root folder: </p> <pre><code>drush cr\n</code></pre>"},{"location":"features/smartcd/","title":"SmartCD","text":"<p>The SmartCD feature provides automation to activate/deactive a ddb project environment.</p> <p>Feature configuration (prefixed with <code>smartcd.</code>)</p> <pre><code>| Property | Type | Description |\n| :---------: | :----: | :----------- |\n| `disabled` | boolean&lt;br&gt;`false` | Should this feature be disabled ? |\n</code></pre>"},{"location":"features/smartcd/#smartcd-automation-on-linuxunix","title":"SmartCD automation on Linux/Unix","text":"<p>Instead of manually launching the command <code>$(ddb run activate)</code> when you are entering a project folder and  <code>$(ddb run deactivate)</code> when leaving it, you can install cxreg/smartcd or inetum-orleans/smartcd to automate this process.</p> <p>As developers, we are lazy. So, we have automated generation of <code>.bash_enter</code> and <code>.bash_leave</code> files. With this feature, you can cd from one project to another without thinking about updating environment  variable as it does it for you.</p> <p>This feature is enabled only if SmartCD is installed.</p>"},{"location":"features/smartcd/#smartcd-automation-on-windows","title":"SmartCD automation on Windows","text":"<p>Sadly, we have not found any way to automate updates on environment when entering or leaving a folder on windows environments.</p> <p>As Windows is a bit onerous when it comes to generation of commands to execute, we have automated the generation of two files : <code>ddb_activate.bat</code> and <code>ddb_deactivate.bat</code> in the root folder of your project. Those two files will execute the environment updated commands needed to work on you project and switch to another one. </p> <p>The downside is that you must run them manually.</p>"},{"location":"features/symlinks/","title":"Symlinks","text":"<p>This feature is a way to automatically create symlinks in your project.</p> <p>One of the common use we have is project with configurations for each environment (dev, stage, prod). With <code>symlinks</code> feature, depending on environment, the final file is link to the one from environment. </p> <p>Feature configuration (prefixed with <code>symlinks.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>suffixes</code> string[]<code>['.${core.env.current}']</code> A list of filename suffix to include. <code>excludes</code> string[]<code>[]</code> A list of glob of filepath to exclude. Advanced Property Type Description <code>includes</code> string[]<code>['*.${core.env.current}{.*,}']</code> A list of glob of filepath to include. It is automatically generated from <code>suffixes</code>."},{"location":"features/symlinks/#symlink-creation","title":"Symlink creation","text":"<p>Bound to events <code>file:found</code>, <code>file:deleted</code> and <code>file:generated</code>, each file retrieved will be compared to lists of configurations <code>includes</code> and <code>excludes</code> to check it is handled or not.</p> <p>If it is a match, symlink will be generated, or deleted if it is a <code>events.file.deleted</code> event.</p> <p>Create <code>.env</code> symlink from <code>.env.prod</code></p> <p>In many frameworks, we want to have different <code>.env</code> files depending on <code>core.env.current</code> value. </p> <p>Let's say we have created a <code>.env.prod</code> which contains production environment configuration. Instead of manually copying the file or creating a symlink, this will create <code>.env</code> symlink pointing to <code>.env.prod</code>  file if <code>core.env.current</code> is set to <code>prod</code>.</p> <p>Tip</p> <p>It can also be chained with any other ddb template generation, such as jinja and ytt</p>"},{"location":"features/traefik/","title":"Traefik","text":"<p>One component which we often use on our dev environment is Traefik as reverse proxy. It allows us to run dockerized projects and access them in our browser using <code>project.test</code> as DNS for example.</p> <p>The feature does not install traefik or handle the configuration of your host to map the DNS entry to the IP, but it handles the generation of traefik configuration file for your project if there is certificates for HTTPS  access.</p> <p>Feature configuration (prefixed with <code>traefik.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>extra_services</code> dict[string, ExtraService]<code>[]</code> A dict of ExtraService configuration. Advanced Property Type Description <code>certs_directory</code> string<code>${core.path.home}/certs</code> Custom certificates location. <code>mapped_certs_directory</code> string<code>${core.path.home}/certs</code> Traefik container custom certificates location. <code>config_directory</code> string<code>${core.path.home}/traefik/config</code> Traefik configuration directory. <code>ssl_config_template</code> string<code>&lt;Jinja template&gt;</code> The Jinja template for the traefik configuration file registering CFSSL SSL certificates. This template can be a template string, or a link to a file containing the template, prefixed with <code>http(s)://</code> for web files, or <code>file://</code> for local ones. <code>extra_services_config_template</code> string<code>&lt;Jinja template&gt;</code> The Jinja template for extra-services configuration file. This template can be a template string, or a link to a file containing the template, prefixed with <code>http(s)://</code> for web files, or <code>file://</code> for local ones. <p>ExtraService configuration (used in <code>traefik.extra_services</code>)</p> Simple Property Type Description <code>domain</code> string* Domain to use for SSL certificate generation. <code>url</code> string* URL to access the service to proxy from traefik container. <code>https</code> boolean Use http and/or https to expose the service. If <code>None</code>, it is exposed with both http and https. <code>redirect_to_https</code> boolean<code>${docker.reverse_proxy.redirect_to_https}</code> If <code>https</code> is <code>None</code> and <code>redirect_to_https</code> is <code>True</code>, requesting the http url of the service will reply with a temporary redirect to https. Advanced Property Type Description <code>path_prefix</code> string The traefik prefix path. You should customize it only if you have to support sub folder on the domain. <code>redirect_to_path_prefix</code> boolean The traefik prefix path. You should customize it only if you have to support sub folder on the domain. <code>rule</code> string<code>Host(`{{_local.domain}}`)</code> Custom certificates location."},{"location":"features/traefik/#include-a-service-running-outside-of-docker-compose","title":"Include a service running outside of docker compose","text":"<p>If you need to register an external service into your docker network, you should define an <code>external_service</code> entry.</p> <p>It can be used when a service is running outside the docker network, like on another Machine or on the developer host.</p> <p>Declared extra services make them join the docker network so it behaves like a docker compose service and brings all  traefik reverse-proxy features (SSL support, domain name, ...).</p> <p>Jinja templating is available for <code>url</code>, <code>domain</code> and <code>rule</code> fields, with the usual configuration as data context,  and additional <code>_local</code> dict containing the extra_service entry configuration.</p> <p>Bring back a dockerized service inside your IDE</p> <p>Running the server component inside the developer editor may more convenient in some cases.</p> <p>If the application is running on port 8080 right on the host, you can write this kind of configuration:</p> <pre><code>traefik:\n  extra_services:\n    api:\n      domain: api.{{core.domain.sub}}.{{core.domain.ext}}\n      url: http://{{docker.debug.host}}:8080\n</code></pre> <p>It will expose the server component throw the traefik docker network, with the domain name and HTTPS support.</p>"},{"location":"features/traefik/#custom-certificates-feature","title":"Custom certificates feature","text":"<p>If your project have certificates for SSL access, Traefik needs a bit a configuration in order to use them.</p> <p>This is done on <code>ddb configure</code> command. For instance, if your have set <code>docker.reverse-proxy.certresolver</code> with <code>null</code>  value in your <code>docker-compose.yml.jsonnet</code> (check feature jsonnet for more details),  it will create a label <code>ddb.emit.certs:generate: &lt;domain&gt;</code>.</p> <p>This label emit a <code>certs:generate</code> for given <code>domain</code>, and it is processed by certs feature to generate a  custom SSL certificate.</p> <p>Then, <code>certs:available</code> event is triggered and handled by traefik feature to install this certificate in the  traefik configuration for given <code>domain</code>.</p> <p>For some reason, you might want to remove HTTPS on your project and move back to HTTP. </p> <p>This is done on <code>ddb configure</code> command. If you define <code>docker.reverse-proxy.certresolver</code> value to 'letsencrypt', or set <code>traefik.https</code> to <code>False</code>, it is  detected that you removed it. </p> <p>The <code>certs:remove</code> event is then triggered and handled by certs feature to remove it.</p> <p>Then, <code>certs:removed</code> event is triggered and handler by traefik feature to uninstall this certificate from traefik  configuration, so there's no more certificate defined for the given domain.</p>"},{"location":"features/version/","title":"Version","text":"<p>Version feature extracts information about the version of your project from local git repository.</p> <p>Feature configuration (prefixed with <code>version.</code>)</p> Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>branch</code> string<code>&lt;current git branch&gt;</code> The current branch of the project. <code>hash</code> string<code>&lt;current git hash&gt;</code> The hash of the current commit. <code>short_hash</code> string<code>&lt;current git short hash&gt;</code> The short version of the hash of the current commit. <code>tag</code> string<code>&lt;current git tag&gt;</code> The current git tag. <code>version</code> string<code>&lt;current project version&gt;</code> The current project version."},{"location":"features/ytt/","title":"ytt","text":"<p>YTT is a template engine dedicated to yaml.</p> <p>Feature configuration (prefixed with <code>ytt.</code>)</p> Simple Property Type Description <code>disabled</code> boolean<code>false</code> Should this feature be disabled ? <code>suffixes</code> string[]<code>['.ytt']</code> A list of filename suffix to include. <code>extensions</code> string[]<code>['yaml', 'yml', '']</code> A list of glob of supported extension. <code>excludes</code> string[]<code>[]</code> A list of glob of filepath to exclude. <code>args</code> string[]<code>[]</code> A list of arguments to pass to ytt. <code>depends_suffixes</code> string[]<code>['.data', '.overlay']</code> File suffix to use for ytt dependency files. Advanced Property Type Description <code>includes</code> string[]<code>['*.ytt{.yaml,.yml,}']</code> A list of glob of filepath to include. It is automatically generated from <code>suffixes</code> and <code>extensions</code>. <code>keywords</code> string[] <code>keywords_escape_format</code> string[]<code>%s_</code>"},{"location":"guides/psql-symfony-vue/","title":"Guide","text":"<p>PostgreSQL, Symfony, VueJS</p> <p>This guide sources are available on github.</p>"},{"location":"guides/psql-symfony-vue/#create-empty-project-directory","title":"Create empty project directory","text":"<p>First of all, you need an empty directory for your project.</p> <pre><code>mkdir ddb-guide\ncd ddb-guide\n</code></pre>"},{"location":"guides/psql-symfony-vue/#setup-database","title":"Setup database","text":"<p>You should now setup the database container. Create <code>docker-compose.yml.jsonnet</code> file, and add the following content:</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n  services: {\n    db: ddb.Image(\"postgres\")\n  }\n})\n</code></pre> <p>Jsonet, a data templating language</p> <p>Instead of defining containers right inside <code>docker-compose.yml</code> with yaml, ddb is using Jsonnet, a data templating language. Inside the jsonnet file, a library is imported to bring handy features and consistent behavior for all containers while reducing verbosity.</p> <p>Jsonet is embedded into ddb</p> <p>Jsonnet is embedded into ddb. You only have to use the right file extension for ddb to  process it through the appropriate template engine.</p> <p>Other template languages are supported</p> <p>ddb embeds other templating languages, like  Jinja and ytt. But for building the  <code>docker-compose.yml</code> file, Jsonnet is the best choice and brings access to all features  from ddb.</p> <p>Run the ddb <code>configure</code> command</p> <pre><code>ddb configure\n</code></pre> <p>Commands</p> <p>ddb is a command line tool, and implements many commands. <code>configure</code> is the main one. It configures the  whole project based on available files in the project directory.</p> <p><code>docker-compose.yml</code> file has been generated.</p> <pre><code>networks: {}\nservices:\n  db:\n    image: postgres\n    init: true\n    restart: 'no'\nvolumes: {}\n</code></pre> <p>.gitignore automation for generated files</p> <p>You may have noticed that a <code>.gitignore</code> has also been generated, to exclude <code>docker-compose.yml</code>. ddb may generates many files from templates. When ddb generates a file, it will always be added to the <code>.gitignore</code> file.</p> <p>Launch the stack with docker compose, and check database logs.</p> <pre><code>docker compose up -d\ndocker compose logs db\n</code></pre> <p>Sadly, there's an error in logs and container has stopped. You only have to define a database password with environment  variable <code>POSTGRES_PASSWORD</code>.</p> <p>Add this environment variable to <code>docker-compose.yml.jsonnet</code> template.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n  services: {\n    db: ddb.Image(\"postgres\") +\n    {\n      environment+: {POSTGRES_PASSWORD: \"ddb\"}\n    }\n  }\n})\n</code></pre> <p>Jsonnet</p> <p>You may feel uncomfortable at first with Jsonnet, but this is a great tool and it brings a  huge value to ddb.</p> <p>Here, we are merging the json object returned by <code>ddb.Image(\"postgres\")</code> with another object containing an <code>environment</code> key with environment variable values. </p> <p><code>+</code> behind <code>environment</code> key name means that values from the new object are appended to values from the source one,  instead of beeing replaced.</p> <p>To fully understand syntax and capabilities of jsonnet, you should take time to learn it.</p> <p>Run <code>configure</code> command again.</p> <pre><code>ddb configure\n</code></pre> <p>The generated <code>docker-compose.yml</code> file should now look like this:</p> <pre><code>networks: {}\nservices:\n  db:\n    environment:\n      POSTGRES_PASSWORD: ddb\n    image: postgres\n    init: true\n    restart: 'no'\nvolumes: {}\n</code></pre> <pre><code>docker compose up -d\ndocker compose logs db\n</code></pre> <p>The database should be up now ! Great :)</p>"},{"location":"guides/psql-symfony-vue/#start-watch-mode","title":"Start watch mode","text":"<p>As you now understand how ddb basics works, you may feel that running the <code>ddb configure</code> after each change is annoying.</p> <p>I'm pretty sure you want to stay a happy developer, so open a new interpreter, cd inside project directory, and run  <code>configure</code> command with <code>--watch</code> flag.</p> <pre><code>ddb --watch configure\n</code></pre> <p>ddb is now running and listening for file events inside the project. </p> <p>Try to change database password inside the <code>docker-compose.yml.jsonnet</code> template file, it will immediately refresh  <code>docker-compose.yml</code>. That's what we call Watch mode.</p>"},{"location":"guides/psql-symfony-vue/#add-a-named-volume","title":"Add a named volume","text":"<p>As you may already know, you need to setup a volume for data to be persisted if the container is destroyed. </p> <p>Let's stop and destroy all containers for now.</p> <pre><code>docker compose down\n</code></pre> <p>Map a named volume to the <code>db</code> service inside <code>docker-compose.yml.jsonnet</code>.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n    services: {\n        db: ddb.Image(\"postgres\") +\n          {\n            environment+: {POSTGRES_PASSWORD: \"ddb\"},\n            volumes+: ['db-data:/var/lib/postgresql/data']\n          }\n    }\n})\n</code></pre> <p>Thanks to watch mode, changes are immediately generated in <code>docker-compose.yml</code> file</p> <pre><code>networks: {}\nservices:\n  db:\n    environment:\n      POSTGRES_PASSWORD: ddb\n    image: postgres\n    init: true\n    restart: 'no'\n    volumes:\n    - db-data:/var/lib/postgresql/data\nvolumes:\n  db-data: {}\n</code></pre> <p><code>db-data</code> volume is now mapped on <code>/var/lib/postgresql/data</code> inside <code>db</code> service. And <code>db-data</code> volume  has also been declared in the main volumes section ! Magic :)</p> <p>In fact, it's not so magic</p> <p>Those automated behavior provided by <code>docker-compose.yml.jsonnet</code>, like <code>init</code> and <code>restart</code> on each service,  and global <code>volumes</code> declaration, are handled by ddb jsonnet library through <code>ddb.Compose()</code> function.</p> <p>For more information, check Jsonnet Feature section.</p>"},{"location":"guides/psql-symfony-vue/#register-binary-from-image","title":"Register binary from image","text":"<p>Database docker image contains binaries that you may need to run, like ...</p> <ul> <li><code>psql</code> - PostgreSQL client binary.</li> <li><code>pg_dump</code> - PostgreSQL command line tool to export data to a file.</li> </ul> <p>With docker compose or raw docker, it may be really painful to find out the right <code>docker compose</code> command to run those  binary from your local environment. You may also have issues to read input data file, or to write output data file.</p> <p>With ddb, you can register those binaries right into <code>docker-compose.yml.jsonnet</code> to make them accessible from your local  environment.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n  services: {\n    db: ddb.Image(\"postgres\") +\n        ddb.Binary(\"psql\", \"/project\", \"psql --dbname=postgresql://postgres:ddb@db/postgres\") +\n        ddb.Binary(\"pg_dump\", \"/project\", \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\") +\n        {\n          environment+: {POSTGRES_PASSWORD: \"ddb\"},\n          volumes+: [\n            'db-data:/var/lib/postgresql/data',\n            ddb.path.project + ':/project'\n          ]\n        }\n  }\n})\n</code></pre> <p>You should notice that some binary files have been generated in <code>.bin</code> directory : <code>psql</code> and <code>pg_dump</code>.</p> <p>Those binaries are available right now on your local environment. You can check their version.</p> <pre><code>.bin/psql --version\n.bin/pg_dump --version\n</code></pre> <p>But ... What the ... Where is all the docker hard stuff ?</p> <p>Of course, the docker hard stuff is still there. But it's hidden. ddb generates some shims for those binaries  available in docker image, so you fell like those binaries are now installed on the project. But when invoking those  binary shims, it creates a temporary container for the command's lifetime.</p> <p>Current working directory is mapped</p> <p>When registering binary from jsonnet this way, the project directory on your local environment should be mounted to  <code>/project</code> inside the container. The working directory of the container is mapped to the working directory of your  local environment. This allow ddb to match working directory from local environment and container, so you are able to access any files through a natural process. </p> <p>Default arguments</p> <p><code>--dbname=postgresql://postgres:ddb@db/postgres</code> is added as a default argument to both command, so the commands won't require any connection settings.</p> <p>To bring <code>psql</code> and <code>pg_dump</code> shims into the path, you have to activate the project environment into your interpreter.</p> <pre><code>$(ddb activate)\n</code></pre> <p><code>.bin</code> directory is now in the interpreter's <code>PATH</code>, so <code>psql</code> and <code>pg_dump</code> are available anywhere.</p> <pre><code>psql --version\npg_dump --version\n</code></pre> <p>Let's try to perform a dump with <code>pg_dump</code>.</p> <pre><code>pg_dump -f dump.sql\n</code></pre> <p>Great, the dump.sql file appears in your working directory ! Perfect. </p> <p>But if you check carefully your project directory, there's a problem here ! The dump file has been generated, but it is  owned by <code>root</code>. You are not allowed to write or delete this file now. </p> <p>Thanks to <code>sudo</code>, you can do still delete it.</p> <pre><code>sudo rm dump.sql\n</code></pre> <p>But this suck ... As a developer, you are really disappointed ... And you are right. Nobody wants a file to be owned by <code>root</code> inside the project directory.</p>"},{"location":"guides/psql-symfony-vue/#workaround-permission-issues","title":"Workaround permission issues","text":"<p>To workaround those permission issues, ddb has automated the installation of fixuid inside a Dockerfile.</p> <p>Docker and permission issues</p> <p>Permission issues are a common pitfall while using docker on development environments. They are related to the way  docker works and cannot really be fixed once for all.</p> <p>As you know, ddb like templates, so you are going to use Jinja for all <code>Dockerfile</code>  files.</p> <p>By convention, custom <code>Dockerfile.jinja</code> lies in <code>.docker/&lt;image&gt;</code> directory, where <code>&lt;image&gt;</code> is to be replaced  with effective image name.</p> <p>First step, create <code>.docker/postgres/Dockerfile.jinja</code> from postgres base image.</p> <pre><code>FROM postgres\nUSER postgres\n</code></pre> <p>Second step, create <code>.docker/postgres/fixuid.yml</code> file.</p> <pre><code>user: postgres\ngroup: postgres\npaths:\n  - /\n  - /var/lib/postgresql/data\n</code></pre> <p>Fixuid</p> <p>Fixuid change <code>uid</code> and <code>gid</code> of the container user to match the host user, and it changes files ownerships as  declared in <code>fixuid.yml</code> configuration file. </p> <p>Most of the time, <code>user</code> and <code>group</code> defined in the configuration file should match the user defined in Dockerfile,  and <code>paths</code> should match the root directory and volume directories.</p> <p>When a <code>fixuid.yml</code> file is available next to a Dockerfile, ddb generates fixuid installation instructions into the  <code>Dockerfile</code>, and entrypoint is changed to run fixuid before the default entrypoint.  </p> <p>Last step, change in <code>docker-compose.yml.jsonnet</code> the service definition to use the newly created Dockerfile (<code>ddb.Image(\"postgres\")</code> replaced to <code>ddb.Build(\"postgres\")</code>), and set <code>user</code> to the host user uid/gid (<code>ddb.User()</code>).</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n    services: {\n        db: ddb.Build(\"postgres\") + ddb.User() +\n            ddb.Binary(\"psql\", \"/project\", \"psql --dbname=postgresql://postgres:ddb@db/postgres\") +\n            ddb.Binary(\"pg_dump\", \"/project\", \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\") +\n          {\n            environment+: {POSTGRES_PASSWORD: \"ddb\"},\n            volumes+: [\n          'db-data:/var/lib/postgresql/data',\n          ddb.path.project + ':/project'\n            ]\n          }\n    }\n})\n</code></pre> <p>Stop containers, destroy data from existing database, and start again.</p> <pre><code>docker compose down -v\ndocker compose up -d\n</code></pre> <p>Perform the dump.</p> <pre><code>pg_dump -f dump.sql\n</code></pre> <p><code>dump.sql</code> is now owned by your own user, and as a developer, you are happy again :)</p>"},{"location":"guides/psql-symfony-vue/#setup-php-apache-and-symfony-skeleton","title":"Setup PHP, Apache and Symfony Skeleton","text":"<p>Then, we need to setup PHP FPM with it's related web server Apache.</p> <p>So, we are creating a new <code>php</code> service inside <code>docker-compose.yml.jsonnet</code>, based on a Dockerfile build.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n    services: {\n        ...\n        php: ddb.Build(\"php\") +\n          ddb.User() +\n          {\n          volumes+: [\n            ddb.path.project + \":/var/www/html\",\n            \"php-composer-cache:/composer/cache\",\n            \"php-composer-vendor:/composer/vendor\"\n          ]\n        }\n})\n</code></pre> <p>And the related <code>Dockerfile.jinja</code> inside <code>.docker/php</code> directory.</p> <pre><code>FROM php:8.2-fpm\n\nRUN yes | pecl install xdebug &amp;&amp; docker-php-ext-enable xdebug\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\nlibpq-dev \\\n&amp;&amp; docker-php-ext-install pdo pdo_pgsql\nRUN rm -rf /var/lib/apt/lists/*\n\nENV COMPOSER_HOME /composer\nENV PATH /composer/vendor/bin:$PATH\nENV COMPOSER_ALLOW_SUPERUSER 1\n\nCOPY --from=composer /usr/bin/composer /usr/bin/composer\nRUN apt-get update -y &amp;&amp;\\\n apt-get install -y git zip unzip &amp;&amp;\\\n rm -rf /var/lib/apt/lists/*\n\nRUN mkdir -p \"$COMPOSER_HOME/cache\" \\\n&amp;&amp; mkdir -p \"$COMPOSER_HOME/vendor\" \\\n&amp;&amp; chown -R www-data:www-data $COMPOSER_HOME \\\n&amp;&amp; chown -R www-data:www-data /var/www\n\nVOLUME /composer/cache\n</code></pre> <p>And <code>fixuid.yml</code> to fix file permission issues.</p> <pre><code>user: www-data\ngroup: www-data\npaths:\n  - /\n  - /composer/cache\n</code></pre> <p>Then build the docker image with <code>docker compose build</code>.</p> <p>Composer has been installed in the image, so let's make it available by registering a binary into  <code>docker-compose.yml.jsonnet</code>. We can also register the <code>php</code> binary for it to be available locally too.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nddb.Compose({\n    services: {\n        ...\n        php: ddb.Build(\"php\") +\n             ddb.User() +\n             ddb.Binary(\"composer\", \"/var/www/html\", \"composer\") +\n             ddb.Binary(\"php\", \"/var/www/html\", \"php\") +\n             ddb.XDebug() +\n             {\n              volumes+: [\n                 ddb.path.project + \":/var/www/html\",\n                 \"php-composer-cache:/composer/cache\",\n                 \"php-composer-vendor:/composer/vendor\"\n              ]\n             }\n        },\n})\n</code></pre> <p>And activate the project, with <code>$(ddb activate)</code>. The composer command in now available right in your PATH.</p> <pre><code>$ composer --version\nComposer version 2.6.5 2023-10-06 10:11:52\n\n$ php --version\nPHP 8.2.11 (cli) (built: Sep 30 2023 02:26:43) (NTS)\nCopyright (c) The PHP Group\nZend Engine v4.2.11, Copyright (c) Zend Technologies\n    with Xdebug v3.2.2, Copyright (c) 2002-2023, by Derick Rethans\n</code></pre> <p>Now PHP and composer are available, you can generate the symfony skeleton inside a <code>backend</code> directory.</p> <pre><code>composer create-project symfony/skeleton backend\n</code></pre> <p>We also need a <code>web</code> service for Apache configured with PHP. Here's the <code>docker-compose.yml.jsonnet</code> part.</p> <pre><code>local ddb = import 'ddb.docker.libjsonnet';\n\nlocal domain_ext = std.extVar(\"core.domain.ext\");\nlocal domain_sub = std.extVar(\"core.domain.sub\");\n\nlocal domain = std.join('.', [domain_sub, domain_ext]);\n\nddb.Compose({\n    services: {\n        ...\n        web: ddb.Build(\"web\") +\n             ddb.VirtualHost(\"80\", domain)\n             {\n                  volumes+: [\n                     ddb.path.project + \":/var/www/html\",\n                     ddb.path.project + \"/.docker/web/apache.conf:/usr/local/apache2/conf/custom/apache.conf\",\n                  ]\n             },\n})\n</code></pre> <p>Use std.extVar(...) inside jsonnet to read a configuration property</p> <p>As you can see here, we are using jsonnet features to build the domain name and setup the traefik configuration for  the virtualhost. Configuration properties are available inside all template engines and can be listed with  <code>ddb config --variables</code></p> <p>As with the <code>php</code> service, a <code>.docker/web/Dockerfile.jinja</code> is created to define the image build.</p> <pre><code>FROM httpd:2.4\n\nRUN mkdir -p /usr/local/apache2/conf/custom \\\n&amp;&amp; mkdir -p /var/www/html \\\n&amp;&amp; sed -i '/LoadModule proxy_module/s/^#//g' /usr/local/apache2/conf/httpd.conf \\\n&amp;&amp; sed -i '/LoadModule proxy_fcgi_module/s/^#//g' /usr/local/apache2/conf/httpd.conf \\\n&amp;&amp; echo &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; echo 'Include conf/custom/*.conf' &gt;&gt; /usr/local/apache2/conf/httpd.conf\n\nRUN sed -i '/LoadModule headers_module/s/^#//g' /usr/local/apache2/conf/httpd.conf\nRUN sed -i '/LoadModule rewrite_module/s/^#//g' /usr/local/apache2/conf/httpd.conf\n</code></pre> <p><code>apache.conf</code> specified in docker compose volume mount is also generated from a jinja file, <code>apache.conf.jinja</code>. It is  used to inject domain name and docker compose network name, for the domain to be centralized into ddb.yml   configuration and ease various environment deployements (stage, prod).</p> <pre><code>&lt;VirtualHost *:80&gt;\n  ServerAdmin webmaster@{{core.domain.sub}}.{{core.domain.ext}}\n  ServerName api.{{core.domain.sub}}.{{core.domain.ext}}\n  DocumentRoot /var/www/html/backend/public\n\n  &lt;Directory \"/var/www/html/backend/public/\"&gt;\n    DirectoryIndex index.php\n\n    AllowOverride All\n    Order allow,deny\n    Allow from all\n    Require all granted\n\n    # symfony configuration from https://github.com/symfony/recipes-contrib/blob/master/symfony/apache-pack/1.0/public/.htaccess\n\n    # By default, Apache does not evaluate symbolic links if you did not enable this\n    # feature in your server configuration. Uncomment the following line if you\n    # install assets as symlinks or if you experience problems related to symlinks\n    # when compiling LESS/Sass/CoffeScript assets.\n    # Options FollowSymlinks\n\n    # Disabling MultiViews prevents unwanted negotiation, e.g. \"/index\" should not resolve\n    # to the front controller \"/index.php\" but be rewritten to \"/index.php/index\".\n    &lt;IfModule mod_negotiation.c&gt;\n        Options -MultiViews\n    &lt;/IfModule&gt;\n\n    &lt;IfModule mod_rewrite.c&gt;\n        RewriteEngine On\n\n        # Determine the RewriteBase automatically and set it as environment variable.\n        # If you are using Apache aliases to do mass virtual hosting or installed the\n        # project in a subdirectory, the base path will be prepended to allow proper\n        # resolution of the index.php file and to redirect to the correct URI. It will\n        # work in environments without path prefix as well, providing a safe, one-size\n        # fits all solution. But as you do not need it in this case, you can comment\n        # the following 2 lines to eliminate the overhead.\n        RewriteCond %{REQUEST_URI}::$1 ^(/.+)/(.*)::\\2$\n        RewriteRule ^(.*) - [E=BASE:%1]\n\n        # Sets the HTTP_AUTHORIZATION header removed by Apache\n        RewriteCond %{HTTP:Authorization} .\n        RewriteRule ^ - [E=HTTP_AUTHORIZATION:%{HTTP:Authorization}]\n\n        # Redirect to URI without front controller to prevent duplicate content\n        # (with and without `/index.php`). Only do this redirect on the initial\n        # rewrite by Apache and not on subsequent cycles. Otherwise we would get an\n        # endless redirect loop (request -&gt; rewrite to front controller -&gt;\n        # redirect -&gt; request -&gt; ...).\n        # So in case you get a \"too many redirects\" error or you always get redirected\n        # to the start page because your Apache does not expose the REDIRECT_STATUS\n        # environment variable, you have 2 choices:\n        # - disable this feature by commenting the following 2 lines or\n        # - use Apache &gt;= 2.3.9 and replace all L flags by END flags and remove the\n        #   following RewriteCond (best solution)\n        RewriteCond %{ENV:REDIRECT_STATUS} ^$\n        RewriteRule ^index\\.php(?:/(.*)|$) %{ENV:BASE}/$1 [R=301,L]\n\n        # If the requested filename exists, simply serve it.\n        # We only want to let Apache serve files and not directories.\n        RewriteCond %{REQUEST_FILENAME} -f\n        RewriteRule ^ - [L]\n\n        # Rewrite all other queries to the front controller.\n        RewriteRule ^ %{ENV:BASE}/index.php [L]\n    &lt;/IfModule&gt;\n\n    &lt;IfModule !mod_rewrite.c&gt;\n        &lt;IfModule mod_alias.c&gt;\n            # When mod_rewrite is not available, we instruct a temporary redirect of\n            # the start page to the front controller explicitly so that the website\n            # and the generated links can still be used.\n            RedirectMatch 307 ^/$ /index.php/\n            # RedirectTemp cannot be used instead\n        &lt;/IfModule&gt;\n    &lt;/IfModule&gt;\n  &lt;/Directory&gt;\n\n  SetEnvIf Authorization \"(.*)\" HTTP_AUTHORIZATION=$1\n\n  &lt;FilesMatch \\.php$&gt;\n      SetHandler \"proxy:fcgi://php.{{docker.compose.network_name}}:9000\"\n  &lt;/FilesMatch&gt;\n&lt;/VirtualHost&gt;\n</code></pre> <p>And now, we are ready start all containers : <code>docker compose up -d</code>. </p> <p>Run <code>ddb info</code> command to check the URL of your virtualhost for the web service.</p> <p>You should be able to view Symfony landing page at  http://api.ddb-quickstart.test and  https://api.ddb-quickstart.test.</p> <p>You may have to restart traefik container</p> <p>If you have some issues with certificate validity on the https:// url, you may need to restart traefik container :  <code>docker restart traefik</code>.</p>"},{"location":"guides/psql-symfony-vue/#setup-vuejs-and-vue-cli","title":"Setup VueJS and Vue CLI","text":"<p>To be continued</p>"}]}